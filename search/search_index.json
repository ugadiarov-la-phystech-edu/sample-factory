{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#sample-factory","title":"Sample Factory","text":"<p>High-throughput reinforcement learning codebase. Resources:</p> <ul> <li> <p>Paper: https://arxiv.org/abs/2006.11751</p> </li> <li> <p>Discord: https://discord.gg/BCfHWaSMkr</p> </li> <li> <p>Twitter (for updates): @petrenko_ai</p> </li> </ul>"},{"location":"#what-is-sample-factory","title":"What is Sample Factory?","text":"<p>Sample Factory is one of the fastest RL libraries focused on very efficient synchronous and asynchronous implementations of policy gradients (PPO). </p> <p>Sample Factory is thoroughly tested and used by many researchers and practitioners. Our implementation is known to reach state-of-the-art (SOTA) performance across a wide range of domains, while minimizing the required training time and hardware requirements. Clips below demonstrate ViZDoom, IsaacGym, DMLab-30, Megaverse, Mujoco, and Atari agents trained with Sample Factory:</p> <p> </p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Highly optimized algorithm architecture for maximum learning throughput</li> <li>Synchronous and asynchronous training regimes</li> <li>Serial (single-process) mode for easy debugging</li> <li>Optimal performance in both CPU-based and GPU-accelerated environments</li> <li>Single- &amp; multi-agent training, self-play, supports training multiple policies at once on one or many GPUs</li> <li>Population-Based Training (PBT)</li> <li>Discrete, continuous, hybrid action spaces</li> <li>Vector-based, image-based, dictionary observation spaces</li> <li>Automatically creates a model architecture by parsing action/observation space specification. Supports custom model architectures</li> <li>Library is designed to be imported into other projects, custom environments are first-class citizens</li> <li>Detailed WandB and Tensorboard summaries, custom metrics</li> <li>HuggingFace \ud83e\udd17 integration (upload trained models and metrics to the Hub)</li> <li>Multiple example environment integrations with tuned parameters and trained models</li> </ul>"},{"location":"#next-steps","title":"Next steps","text":"<p>Check out the following guides to get started:</p> <ul> <li>Installation</li> <li>Basic Usage</li> </ul>"},{"location":"01-get-started/basic-usage/","title":"Basic Usage","text":""},{"location":"01-get-started/basic-usage/#usage-examples","title":"Usage examples","text":"<p>Use command line to train an agent using one of the existing integrations, e.g. Mujoco (might need to run <code>pip install sample-factory[mujoco]</code>):</p> <pre><code>python -m sf_examples.mujoco.train_mujoco --env=mujoco_ant --experiment=Ant --train_dir=./train_dir\n</code></pre> <p>Stop the experiment when the desired performance is reached and then evaluate the agent:</p> <pre><code>python -m sf_examples.mujoco.enjoy_mujoco --env=mujoco_ant --experiment=Ant --train_dir=./train_dir\n</code></pre> <p>Do the same in a pixel-based environment such as VizDoom (might need to run <code>pip install sample-factory[vizdoom]</code>, please also see docs for VizDoom-specific instructions):</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_basic --experiment=DoomBasic --train_dir=./train_dir --num_workers=16 --num_envs_per_worker=10 --train_for_env_steps=1000000\npython -m sf_examples.vizdoom.enjoy_vizdoom --env=doom_basic --experiment=DoomBasic --train_dir=./train_dir\n</code></pre>"},{"location":"01-get-started/basic-usage/#monitoring-experiments","title":"Monitoring experiments","text":"<p>Monitor any running or completed experiment with Tensorboard:</p> <p><pre><code>tensorboard --logdir=./train_dir\n</code></pre> (or see the docs for WandB integration).</p>"},{"location":"01-get-started/basic-usage/#next-steps","title":"Next steps","text":"<ul> <li>Read more about configuring experiments in the Configuration guide.</li> <li>Follow the instructions in the Customizing guide to train an agent in your own environment.</li> </ul>"},{"location":"01-get-started/installation/","title":"Installation","text":"<p>Just install from PyPI:</p> <p><code>pip install sample-factory</code></p> <p>SF is known to work on Linux and macOS. There is no Windows support at this time.</p>"},{"location":"01-get-started/installation/#install-from-sources","title":"Install from sources","text":"<pre><code>git clone git@github.com:alex-petrenko/sample-factory.git\ncd sample-factory\npip install -e .\n\n# or install with optional dependencies\npip install -e .[dev,mujoco,atari,vizdoom]\n</code></pre>"},{"location":"01-get-started/installation/#environment-support","title":"Environment support","text":"<p>To run Sample Factory with one of the available environment integrations, please refer to the corresponding documentation sections: </p> <ul> <li>Mujoco</li> <li>Atari</li> <li>ViZDoom</li> <li>DeepMind Lab</li> <li>Megaverse</li> <li>Envpool</li> <li>Isaac Gym</li> <li>Quad-Swarm-RL</li> </ul> <p>Sample Factory allows users to easily add custom environments and models, refer to Customizing Sample Factory for more information.</p>"},{"location":"02-configuration/cfg-params/","title":"Full Parameter Reference","text":"<p>The command line arguments / config parameters for training using Sample Factory can be found by running your training script with the <code>--help</code> flag. The list of config parameters below was obtained from running <code>python -m sf_examples.train_gym_env --env=CartPole-v1 --help</code>. These params can be used in any environment. Other environments may have other custom params than can also be viewed with <code>--help</code> flag when running the environment-specific training script.</p> <pre><code>usage: train_gym_env.py [-h] [--algo ALGO] --env ENV [--experiment EXPERIMENT]\n                        [--train_dir TRAIN_DIR]\n                        [--restart_behavior {resume,restart,overwrite}]\n                        [--device {gpu,cpu}] [--seed SEED]\n                        [--num_policies NUM_POLICIES] [--async_rl ASYNC_RL]\n                        [--serial_mode SERIAL_MODE]\n                        [--batched_sampling BATCHED_SAMPLING]\n                        [--num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE]\n                        [--worker_num_splits WORKER_NUM_SPLITS]\n                        [--policy_workers_per_policy POLICY_WORKERS_PER_POLICY]\n                        [--max_policy_lag MAX_POLICY_LAG]\n                        [--num_workers NUM_WORKERS]\n                        [--num_envs_per_worker NUM_ENVS_PER_WORKER]\n                        [--batch_size BATCH_SIZE]\n                        [--num_batches_per_epoch NUM_BATCHES_PER_EPOCH]\n                        [--num_epochs NUM_EPOCHS] [--rollout ROLLOUT]\n                        [--recurrence RECURRENCE]\n                        [--shuffle_minibatches SHUFFLE_MINIBATCHES]\n                        [--gamma GAMMA] [--reward_scale REWARD_SCALE]\n                        [--reward_clip REWARD_CLIP]\n                        [--value_bootstrap VALUE_BOOTSTRAP]\n                        [--normalize_returns NORMALIZE_RETURNS]\n                        [--exploration_loss_coeff EXPLORATION_LOSS_COEFF]\n                        [--value_loss_coeff VALUE_LOSS_COEFF]\n                        [--kl_loss_coeff KL_LOSS_COEFF]\n                        [--exploration_loss {entropy,symmetric_kl}]\n                        [--gae_lambda GAE_LAMBDA]\n                        [--ppo_clip_ratio PPO_CLIP_RATIO]\n                        [--ppo_clip_value PPO_CLIP_VALUE]\n                        [--with_vtrace WITH_VTRACE] [--vtrace_rho VTRACE_RHO]\n                        [--vtrace_c VTRACE_C] [--optimizer {adam,lamb}]\n                        [--adam_eps ADAM_EPS] [--adam_beta1 ADAM_BETA1]\n                        [--adam_beta2 ADAM_BETA2]\n                        [--max_grad_norm MAX_GRAD_NORM]\n                        [--learning_rate LEARNING_RATE]\n                        [--lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch}]\n                        [--lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD]\n                        [--lr_adaptive_min LR_ADAPTIVE_MIN]\n                        [--lr_adaptive_max LR_ADAPTIVE_MAX]\n                        [--obs_subtract_mean OBS_SUBTRACT_MEAN]\n                        [--obs_scale OBS_SCALE]\n                        [--normalize_input NORMALIZE_INPUT]\n                        [--normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]]]\n                        [--decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS]\n                        [--decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER]\n                        [--actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]]]\n                        [--set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY]\n                        [--force_envs_single_thread FORCE_ENVS_SINGLE_THREAD]\n                        [--default_niceness DEFAULT_NICENESS]\n                        [--log_to_file LOG_TO_FILE]\n                        [--experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL]\n                        [--flush_summaries_interval FLUSH_SUMMARIES_INTERVAL]\n                        [--stats_avg STATS_AVG]\n                        [--summaries_use_frameskip SUMMARIES_USE_FRAMESKIP]\n                        [--heartbeat_interval HEARTBEAT_INTERVAL]\n                        [--heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL]\n                        [--train_for_env_steps TRAIN_FOR_ENV_STEPS]\n                        [--train_for_seconds TRAIN_FOR_SECONDS]\n                        [--save_every_sec SAVE_EVERY_SEC]\n                        [--keep_checkpoints KEEP_CHECKPOINTS]\n                        [--load_checkpoint_kind {latest,best}]\n                        [--save_milestones_sec SAVE_MILESTONES_SEC]\n                        [--save_best_every_sec SAVE_BEST_EVERY_SEC]\n                        [--save_best_metric SAVE_BEST_METRIC]\n                        [--save_best_after SAVE_BEST_AFTER]\n                        [--benchmark BENCHMARK]\n                        [--encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]]]\n                        [--encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala}]\n                        [--encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]]]\n                        [--use_rnn USE_RNN] [--rnn_size RNN_SIZE]\n                        [--rnn_type {gru,lstm}]\n                        [--rnn_num_layers RNN_NUM_LAYERS]\n                        [--decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]]]\n                        [--nonlinearity {elu,relu,tanh}]\n                        [--policy_initialization {orthogonal,xavier_uniform,torch_default}]\n                        [--policy_init_gain POLICY_INIT_GAIN]\n                        [--actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS]\n                        [--adaptive_stddev ADAPTIVE_STDDEV]\n                        [--continuous_tanh_scale CONTINUOUS_TANH_SCALE]\n                        [--initial_stddev INITIAL_STDDEV]\n                        [--use_env_info_cache USE_ENV_INFO_CACHE]\n                        [--env_gpu_actions ENV_GPU_ACTIONS]\n                        [--env_gpu_observations ENV_GPU_OBSERVATIONS]\n                        [--env_frameskip ENV_FRAMESKIP]\n                        [--env_framestack ENV_FRAMESTACK]\n                        [--pixel_format PIXEL_FORMAT]\n                        [--use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS]\n                        [--episode_counter EPISODE_COUNTER]\n                        [--with_wandb WITH_WANDB] [--wandb_user WANDB_USER]\n                        [--wandb_project WANDB_PROJECT]\n                        [--wandb_group WANDB_GROUP]\n                        [--wandb_job_type WANDB_JOB_TYPE]\n                        [--wandb_tags [WANDB_TAGS [WANDB_TAGS ...]]]\n                        [--with_pbt WITH_PBT]\n                        [--pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV]\n                        [--pbt_period_env_steps PBT_PERIOD_ENV_STEPS]\n                        [--pbt_start_mutation PBT_START_MUTATION]\n                        [--pbt_replace_fraction PBT_REPLACE_FRACTION]\n                        [--pbt_mutation_rate PBT_MUTATION_RATE]\n                        [--pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP]\n                        [--pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE]\n                        [--pbt_optimize_gamma PBT_OPTIMIZE_GAMMA]\n                        [--pbt_target_objective PBT_TARGET_OBJECTIVE]\n                        [--pbt_perturb_min PBT_PERTURB_MIN]\n                        [--pbt_perturb_max PBT_PERTURB_MAX]\n\noptional arguments:\n  -h, --help            Print the help message (default: False)\n  --algo ALGO           Algorithm to use (default: APPO)\n  --env ENV             Name of the environment to use (default: None)\n  --experiment EXPERIMENT\n                        Unique experiment name. This will also be the name for\n                        the experiment folder in the train dir.If the\n                        experiment folder with this name aleady exists the\n                        experiment will be RESUMED!Any parameters passed from\n                        command line that do not match the parameters stored\n                        in the experiment config.json file will be overridden.\n                        (default: default_experiment)\n  --train_dir TRAIN_DIR\n                        Root for all experiments (default:\n                        /home/runner/work/sample-factory/sample-\n                        factory/train_dir)\n  --restart_behavior {resume,restart,overwrite}\n                        How to handle the experiment if the directory with the\n                        same name already exists. \"resume\" (default) will\n                        resume the experiment, \"restart\" will preserve the\n                        existing experiment folder under a different name\n                        (with \"old\" suffix) and will start training from\n                        scratch, \"overwrite\" will delete the existing\n                        experiment folder and start from scratch. This\n                        parameter does not have any effect if the experiment\n                        directory does not exist. (default: resume)\n  --device {gpu,cpu}    CPU training is only recommended for smaller e.g. MLP\n                        policies (default: gpu)\n  --seed SEED           Set a fixed seed value (default: None)\n  --num_policies NUM_POLICIES\n                        Number of policies to train jointly, i.e. for multi-\n                        agent environments (default: 1)\n  --async_rl ASYNC_RL   Collect experience asynchronously while learning on\n                        the previous batch. This is significantly different\n                        from standard synchronous actor-critic (or PPO)\n                        because not all of the experience will be collected by\n                        the latest policy thus increasing policy lag. Negative\n                        effects of using async_rl can range from negligible\n                        (just grants you throughput boost) to quite serious\n                        where you can consider switching it off. It all\n                        depends how sensitive your experiment is to policy\n                        lag. Envs with complex action spaces and RNN policies\n                        tend to be particularly sensitive. (default: True)\n  --serial_mode SERIAL_MODE\n                        Enable serial mode: run everything completely\n                        synchronously in the same process (default: False)\n  --batched_sampling BATCHED_SAMPLING\n                        Batched sampling allows the data to be processed in\n                        big batches on the rollout worker.This is especially\n                        important for GPU-accelerated vectorized environments\n                        such as Megaverse or IsaacGym. As a downside, in\n                        batched mode we do not support (for now) some of the\n                        features, such as population-based self-play or\n                        inactive agents, plus each batched sampler (rollout\n                        worker) process only collects data for a single\n                        policy. Another issue between batched/non-batched\n                        sampling is handling of infos. In batched mode we\n                        assume that infos is a single dictionary of\n                        lists/tensors containing info for each environment in\n                        a vector. If you need some complex info dictionary\n                        handling and your environment might return dicts with\n                        different keys, on different rollout steps, you\n                        probably need non-batched mode. (default: False)\n  --num_batches_to_accumulate NUM_BATCHES_TO_ACCUMULATE\n                        This parameter governs the maximum number of training\n                        batches the learner can accumulate before further\n                        experience collection is stopped. The default value\n                        will set this to 2, so if the experience collection is\n                        faster than the training, the learner will accumulate\n                        enough minibatches for 2 iterations of training but no\n                        more. This is a good balance between policy-lag and\n                        throughput. When the limit is reached, the learner\n                        will notify the actor workers that they ought to stop\n                        the experience collection until accumulated\n                        minibatches are processed. Set this parameter to 1 to\n                        further reduce policy-lag. If the experience\n                        collection is very non-uniform, increasing this\n                        parameter can increase overall throughput, at the cost\n                        of increased policy-lag. (default: 2)\n  --worker_num_splits WORKER_NUM_SPLITS\n                        Typically we split a vector of envs into two parts for\n                        \"double buffered\" experience collection Set this to 1\n                        to disable double buffering. Set this to 3 for triple\n                        buffering! (default: 2)\n  --policy_workers_per_policy POLICY_WORKERS_PER_POLICY\n                        Number of policy workers that compute forward pass\n                        (per policy) (default: 1)\n  --max_policy_lag MAX_POLICY_LAG\n                        Max policy lag in policy versions. Discard all\n                        experience that is older than this. (default: 1000)\n  --num_workers NUM_WORKERS\n                        Number of parallel environment workers. Should be less\n                        than num_envs and should divide num_envs.Use this in\n                        async mode. (default: 4)\n  --num_envs_per_worker NUM_ENVS_PER_WORKER\n                        Number of envs on a single CPU actor, in high-\n                        throughput configurations this should be in 10-30\n                        range for Atari/VizDoomMust be even for double-\n                        buffered sampling! (default: 2)\n  --batch_size BATCH_SIZE\n                        Minibatch size for SGD (default: 1024)\n  --num_batches_per_epoch NUM_BATCHES_PER_EPOCH\n                        This determines the training dataset size for each\n                        iteration of training. We collect this many\n                        minibatches before performing any SGD. Example: if\n                        batch_size=128 and num_batches_per_epoch=2, then\n                        learner will process 2*128=256 environment transitions\n                        in one training iteration. (default: 1)\n  --num_epochs NUM_EPOCHS\n                        Number of training epochs on a dataset of collected\n                        experiences of size batch_size x num_batches_per_epoch\n                        (default: 1)\n  --rollout ROLLOUT     Length of the rollout from each environment in\n                        timesteps.Once we collect this many timesteps on actor\n                        worker, we send this trajectory to the learner.The\n                        length of the rollout will determine how many\n                        timesteps are used to calculate bootstrappedMonte-\n                        Carlo estimates of discounted rewards, advantages,\n                        GAE, or V-trace targets. Shorter rolloutsreduce\n                        variance, but the estimates are less precise (bias vs\n                        variance tradeoff).For RNN policies, this should be a\n                        multiple of --recurrence, so every rollout will be\n                        splitinto (n = rollout / recurrence) segments for\n                        backpropagation. V-trace algorithm currently requires\n                        thatrollout == recurrence, which what you want most of\n                        the time anyway.Rollout length is independent from the\n                        episode length. Episode length can be both shorter or\n                        longer thanrollout, although for PBT training it is\n                        currently recommended that rollout &lt;&lt; episode_len(see\n                        function finalize_trajectory in actor_worker.py)\n                        (default: 32)\n  --recurrence RECURRENCE\n                        Trajectory length for backpropagation through time.\n                        Default value (-1) sets recurrence to rollout length\n                        for RNNs and to 1 (no recurrence) for feed-forward\n                        nets. If you train with V-trace recurrence should be\n                        equal to rollout length. (default: -1)\n  --shuffle_minibatches SHUFFLE_MINIBATCHES\n                        Whether to randomize and shuffle minibatches between\n                        iterations (this is a slow operation when batches are\n                        large, disabling this increases learner throughput\n                        when training with multiple epochs/minibatches per\n                        epoch) (default: False)\n  --gamma GAMMA         Discount factor (default: 0.99)\n  --reward_scale REWARD_SCALE\n                        Multiply all rewards by this factor before feeding\n                        into RL algorithm.Sometimes the overall scale of\n                        rewards is too high which makes value estimation a\n                        harder regression task.Loss values become too high\n                        which requires a smaller learning rate, etc. (default:\n                        1.0)\n  --reward_clip REWARD_CLIP\n                        Clip rewards between [-c, c]. Default [-1000, 1000]\n                        should mean no clipping for most envs (unless rewards\n                        are very large/small) (default: 1000.0)\n  --value_bootstrap VALUE_BOOTSTRAP\n                        Bootstrap returns from value estimates if episode is\n                        terminated by timeout. More info here:\n                        https://github.com/Denys88/rl_games/issues/128\n                        (default: False)\n  --normalize_returns NORMALIZE_RETURNS\n                        Whether to use running mean and standard deviation to\n                        normalize discounted returns (default: True)\n  --exploration_loss_coeff EXPLORATION_LOSS_COEFF\n                        Coefficient for the exploration component of the loss\n                        function. (default: 0.003)\n  --value_loss_coeff VALUE_LOSS_COEFF\n                        Coefficient for the critic loss (default: 0.5)\n  --kl_loss_coeff KL_LOSS_COEFF\n                        Coefficient for fixed KL loss (as used by Schulman et\n                        al. in https://arxiv.org/pdf/1707.06347.pdf). Highly\n                        recommended for environments with continuous action\n                        spaces. (default: 0.0)\n  --exploration_loss {entropy,symmetric_kl}\n                        Usually the exploration loss is based on maximizing\n                        the entropy of the probability distribution. Note that\n                        mathematically maximizing entropy of the categorical\n                        probability distribution is exactly the same as\n                        minimizing the (regular) KL-divergence between this\n                        distribution and a uniform prior. The downside of\n                        using the entropy term (or regular asymmetric KL-\n                        divergence) is the fact that penalty does not increase\n                        as probabilities of some actions approach zero. I.e.\n                        numerically, there is almost no difference between an\n                        action distribution with a probability epsilon &gt; 0 for\n                        some action and an action distribution with a\n                        probability = zero for this action. For many tasks the\n                        first (epsilon) distribution is preferrable because we\n                        keep some (albeit small) amount of exploration, while\n                        the second distribution will never explore this action\n                        ever again.Unlike the entropy term, symmetric KL\n                        divergence between the action distribution and a\n                        uniform prior approaches infinity when entropy of the\n                        distribution approaches zero, so it can prevent the\n                        pathological situations where the agent stops\n                        exploring. Empirically, symmetric KL-divergence\n                        yielded slightly better results on some problems.\n                        (default: entropy)\n  --gae_lambda GAE_LAMBDA\n                        Generalized Advantage Estimation discounting (only\n                        used when V-trace is False) (default: 0.95)\n  --ppo_clip_ratio PPO_CLIP_RATIO\n                        We use unbiased clip(x, 1+e, 1/(1+e)) instead of\n                        clip(x, 1+e, 1-e) in the paper (default: 0.1)\n  --ppo_clip_value PPO_CLIP_VALUE\n                        Maximum absolute change in value estimate until it is\n                        clipped. Sensitive to value magnitude (default: 1.0)\n  --with_vtrace WITH_VTRACE\n                        Enables V-trace off-policy correction. If this is\n                        True, then GAE is not used (default: False)\n  --vtrace_rho VTRACE_RHO\n                        rho_hat clipping parameter of the V-trace algorithm\n                        (importance sampling truncation) (default: 1.0)\n  --vtrace_c VTRACE_C   c_hat clipping parameter of the V-trace algorithm. Low\n                        values for c_hat can reduce variance of the advantage\n                        estimates (similar to GAE lambda &lt; 1) (default: 1.0)\n  --optimizer {adam,lamb}\n                        Type of optimizer to use (default: adam)\n  --adam_eps ADAM_EPS   Adam epsilon parameter (1e-8 to 1e-5 seem to reliably\n                        work okay, 1e-3 and up does not work) (default: 1e-06)\n  --adam_beta1 ADAM_BETA1\n                        Adam momentum decay coefficient (default: 0.9)\n  --adam_beta2 ADAM_BETA2\n                        Adam second momentum decay coefficient (default:\n                        0.999)\n  --max_grad_norm MAX_GRAD_NORM\n                        Max L2 norm of the gradient vector, set to 0 to\n                        disable gradient clipping (default: 4.0)\n  --learning_rate LEARNING_RATE\n                        LR (default: 0.0001)\n  --lr_schedule {constant,kl_adaptive_minibatch,kl_adaptive_epoch}\n                        Learning rate schedule to use. Constant keeps constant\n                        learning rate throughout training.kl_adaptive*\n                        schedulers look at --lr_schedule_kl_threshold and if\n                        KL-divergence with behavior policyafter the last\n                        minibatch/epoch significantly deviates from this\n                        threshold, lr is apropriatelyincreased or decreased\n                        (default: constant)\n  --lr_schedule_kl_threshold LR_SCHEDULE_KL_THRESHOLD\n                        Used with kl_adaptive_* schedulers (default: 0.008)\n  --lr_adaptive_min LR_ADAPTIVE_MIN\n                        Minimum learning rate (default: 1e-06)\n  --lr_adaptive_max LR_ADAPTIVE_MAX\n                        Maximum learning rate. This is the best value tuned\n                        for IsaacGymEnvs environments such as Ant/Humanoid,\n                        but it can be too high for some other envs. Set this\n                        to 1e-3 if you see instabilities with adaptive LR,\n                        especially if reported LR on Tensorboard reaches this\n                        max value before the instability happens. (default:\n                        0.01)\n  --obs_subtract_mean OBS_SUBTRACT_MEAN\n                        Observation preprocessing, mean value to subtract from\n                        observation (e.g. 128.0 for 8-bit RGB) (default: 0.0)\n  --obs_scale OBS_SCALE\n                        Observation preprocessing, divide observation tensors\n                        by this scalar (e.g. 128.0 for 8-bit RGB) (default:\n                        1.0)\n  --normalize_input NORMALIZE_INPUT\n                        Whether to use running mean and standard deviation to\n                        normalize observations (default: True)\n  --normalize_input_keys [NORMALIZE_INPUT_KEYS [NORMALIZE_INPUT_KEYS ...]]\n                        Which observation keys to use for normalization. If\n                        None, all observation keys are used (be careful with\n                        this!) (default: None)\n  --decorrelate_experience_max_seconds DECORRELATE_EXPERIENCE_MAX_SECONDS\n                        Decorrelating experience serves two benefits. First:\n                        this is better for learning because samples from\n                        workers come from random moments in the episode,\n                        becoming more \"i.i.d\".Second, and more important one:\n                        this is good for environments with highly non-uniform\n                        one-step times, including long and expensive episode\n                        resets. If experience is not decorrelatedthen training\n                        batches will come in bursts e.g. after a bunch of\n                        environments finished resets and many iterations on\n                        the learner might be required,which will increase the\n                        policy-lag of the new experience collected. The\n                        performance of the Sample Factory is best when\n                        experience is generated as more-or-lessuniform stream.\n                        Try increasing this to 100-200 seconds to smoothen the\n                        experience distribution in time right from the\n                        beginning (it will eventually spread out and settle\n                        anyways) (default: 0)\n  --decorrelate_envs_on_one_worker DECORRELATE_ENVS_ON_ONE_WORKER\n                        In addition to temporal decorrelation of worker\n                        processes, also decorrelate envs within one worker\n                        process. For environments with a fixed episode length\n                        it can prevent the reset from happening in the same\n                        rollout for all envs simultaneously, which makes\n                        experience collection more uniform. (default: True)\n  --actor_worker_gpus [ACTOR_WORKER_GPUS [ACTOR_WORKER_GPUS ...]]\n                        By default, actor workers only use CPUs. Changes this\n                        if e.g. you need GPU-based rendering on the actors\n                        (default: [])\n  --set_workers_cpu_affinity SET_WORKERS_CPU_AFFINITY\n                        Whether to assign workers to specific CPU cores or\n                        not. The logic is beneficial for most workloads\n                        because prevents a lot of context switching.However\n                        for some environments it can be better to disable it,\n                        to allow one worker to use all cores some of the time.\n                        This can be the case for some DMLab environments with\n                        very expensive episode resetthat can use parallel CPU\n                        cores for level generation. (default: True)\n  --force_envs_single_thread FORCE_ENVS_SINGLE_THREAD\n                        Some environments may themselves use parallel\n                        libraries such as OpenMP or MKL. Since we parallelize\n                        environments on the level of workers, there is no need\n                        to keep this parallel semantic.This flag uses\n                        threadpoolctl to force libraries such as OpenMP and\n                        MKL to use only a single thread within the\n                        environment.Enabling this is recommended unless you\n                        are running fewer workers than CPU cores.\n                        threadpoolctl has caused a bunch of crashes in the\n                        past, so this feature is disabled by default at this\n                        moment. (default: False)\n  --default_niceness DEFAULT_NICENESS\n                        Niceness of the highest priority process (the\n                        learner). Values below zero require elevated\n                        privileges. (default: 0)\n  --log_to_file LOG_TO_FILE\n                        Whether to log to a file (sf_log.txt in the experiment\n                        folder) or not. If False, logs to stdout only. It can\n                        make sense to disable this in a slow server filesystem\n                        environment like NFS. (default: True)\n  --experiment_summaries_interval EXPERIMENT_SUMMARIES_INTERVAL\n                        How often in seconds we write avg. statistics about\n                        the experiment (reward, episode length, extra\n                        stats...) (default: 10)\n  --flush_summaries_interval FLUSH_SUMMARIES_INTERVAL\n                        How often do we flush tensorboard summaries (set to\n                        higher value for slow NFS-based server filesystems)\n                        (default: 30)\n  --stats_avg STATS_AVG\n                        How many episodes to average to measure performance\n                        (avg. reward etc) (default: 100)\n  --summaries_use_frameskip SUMMARIES_USE_FRAMESKIP\n                        Whether to multiply training steps by frameskip when\n                        recording summaries, FPS, etc. When this flag is set\n                        to True, x-axis for all summaries corresponds to the\n                        total number of simulated steps, i.e. with frameskip=4\n                        the x-axis value of 4 million will correspond to 1\n                        million frames observed by the policy. (default: True)\n  --heartbeat_interval HEARTBEAT_INTERVAL\n                        How often in seconds components send a heartbeat\n                        signal to the runner to verify they are not stuck\n                        (default: 20)\n  --heartbeat_reporting_interval HEARTBEAT_REPORTING_INTERVAL\n                        How often in seconds the runner checks for heartbeats\n                        (default: 180)\n  --train_for_env_steps TRAIN_FOR_ENV_STEPS\n                        Stop after all policies are trained for this many env\n                        steps (default: 10000000000)\n  --train_for_seconds TRAIN_FOR_SECONDS\n                        Stop training after this many seconds (default:\n                        10000000000)\n  --save_every_sec SAVE_EVERY_SEC\n                        Checkpointing rate (default: 120)\n  --keep_checkpoints KEEP_CHECKPOINTS\n                        Number of model checkpoints to keep (default: 2)\n  --load_checkpoint_kind {latest,best}\n                        Whether to load from latest or best checkpoint\n                        (default: latest)\n  --save_milestones_sec SAVE_MILESTONES_SEC\n                        Save intermediate checkpoints in a separate folder for\n                        later evaluation (default=never) (default: -1)\n  --save_best_every_sec SAVE_BEST_EVERY_SEC\n                        How often we check if we should save the policy with\n                        the best score ever (default: 5)\n  --save_best_metric SAVE_BEST_METRIC\n                        Save \"best\" policies based on this metric (just env\n                        reward by default) (default: reward)\n  --save_best_after SAVE_BEST_AFTER\n                        Start saving \"best\" policies after this many env steps\n                        to filter lucky episodes that succeed and dominate the\n                        statistics early on (default: 100000)\n  --benchmark BENCHMARK\n                        Benchmark mode (default: False)\n  --encoder_mlp_layers [ENCODER_MLP_LAYERS [ENCODER_MLP_LAYERS ...]]\n                        In case of MLP encoder, sizes of layers to use. This\n                        is ignored if observations are images. To use this\n                        parameter from command line, omit the = sign and\n                        separate values with spaces, e.g. --encoder_mlp_layers\n                        256 128 64 (default: [512, 512])\n  --encoder_conv_architecture {convnet_simple,convnet_impala,convnet_atari,resnet_impala}\n                        Architecture of the convolutional encoder. See\n                        models.py for details. VizDoom and DMLab examples\n                        demonstrate how to define custom architectures.\n                        (default: convnet_simple)\n  --encoder_conv_mlp_layers [ENCODER_CONV_MLP_LAYERS [ENCODER_CONV_MLP_LAYERS ...]]\n                        Optional fully connected layers after the\n                        convolutional encoder head. (default: [512])\n  --use_rnn USE_RNN     Whether to use RNN core in a policy or not (default:\n                        True)\n  --rnn_size RNN_SIZE   Size of the RNN hidden state in recurrent model (e.g.\n                        GRU or LSTM) (default: 512)\n  --rnn_type {gru,lstm}\n                        Type of RNN cell to use if use_rnn is True (default:\n                        gru)\n  --rnn_num_layers RNN_NUM_LAYERS\n                        Number of RNN layers to use if use_rnn is True\n                        (default: 1)\n  --decoder_mlp_layers [DECODER_MLP_LAYERS [DECODER_MLP_LAYERS ...]]\n                        Optional decoder MLP layers after the policy core. If\n                        empty (default) decoder is identity function.\n                        (default: [])\n  --nonlinearity {elu,relu,tanh}\n                        Type of nonlinearity to use. (default: elu)\n  --policy_initialization {orthogonal,xavier_uniform,torch_default}\n                        NN weight initialization (default: orthogonal)\n  --policy_init_gain POLICY_INIT_GAIN\n                        Gain parameter of PyTorch initialization schemas (i.e.\n                        Xavier) (default: 1.0)\n  --actor_critic_share_weights ACTOR_CRITIC_SHARE_WEIGHTS\n                        Whether to share the weights between policy and value\n                        function (default: True)\n  --adaptive_stddev ADAPTIVE_STDDEV\n                        Only for continuous action distributions, whether\n                        stddev is state-dependent or just a single learned\n                        parameter (default: True)\n  --continuous_tanh_scale CONTINUOUS_TANH_SCALE\n                        Only for continuous action distributions, whether to\n                        use tanh squashing and what scale to use. Applies\n                        tanh(mu / scale) * scale to distribution means.\n                        Experimental. Currently only works with\n                        adaptive_stddev=False (TODO). (default: 0.0)\n  --initial_stddev INITIAL_STDDEV\n                        Initial value for non-adaptive stddev. Only makes\n                        sense for continuous action spaces (default: 1.0)\n  --use_env_info_cache USE_ENV_INFO_CACHE\n                        Whether to use cached env info (default: False)\n  --env_gpu_actions ENV_GPU_ACTIONS\n                        Set to true if environment expects actions on GPU\n                        (i.e. as a GPU-side PyTorch tensor) (default: False)\n  --env_gpu_observations ENV_GPU_OBSERVATIONS\n                        Setting this to True together with non-empty\n                        --actor_worker_gpus will make observations GPU-side\n                        PyTorch tensors. Otherwise data will be on CPU. For\n                        CPU-based envs just set --actor_worker_gpus to empty\n                        list then this parameter does not matter. (default:\n                        True)\n  --env_frameskip ENV_FRAMESKIP\n                        Number of frames for action repeat (frame skipping).\n                        Setting this to &gt;1 will not add any wrappers that will\n                        do frame-skipping, although this can be used in the\n                        environment factory function to add these wrappers or\n                        to tell the environment itself to skip a desired\n                        number of frames i.e. as it is done in VizDoom. FPS\n                        metrics will be multiplied by the frameskip value,\n                        i.e. 100000FPS with frameskip=4 actually corresponds\n                        to 100000/4=25000 samples per second observed by the\n                        policy. Frameskip=1 (default) means no frameskip, we\n                        process every frame. (default: 1)\n  --env_framestack ENV_FRAMESTACK\n                        Frame stacking (only used in Atari, and it is usually\n                        set to 4) (default: 1)\n  --pixel_format PIXEL_FORMAT\n                        PyTorch expects CHW by default, Ray &amp; TensorFlow\n                        expect HWC (default: CHW)\n  --use_record_episode_statistics USE_RECORD_EPISODE_STATISTICS\n                        Whether to use gym RecordEpisodeStatistics wrapper to\n                        keep track of reward (default: False)\n  --episode_counter EPISODE_COUNTER\n                        Add wrapper to each env which will count the number of\n                        episodes for each env. (default: False)\n  --with_wandb WITH_WANDB\n                        Enables Weights and Biases integration (default:\n                        False)\n  --wandb_user WANDB_USER\n                        WandB username (entity). Must be specified from\n                        command line! Also see\n                        https://docs.wandb.ai/quickstart#1.-set-up-wandb\n                        (default: None)\n  --wandb_project WANDB_PROJECT\n                        WandB \"Project\" (default: sample_factory)\n  --wandb_group WANDB_GROUP\n                        WandB \"Group\" (to group your experiments). By default\n                        this is the name of the env. (default: None)\n  --wandb_job_type WANDB_JOB_TYPE\n                        WandB job type (default: SF)\n  --wandb_tags [WANDB_TAGS [WANDB_TAGS ...]]\n                        Tags can help with finding experiments in WandB web\n                        console (default: [])\n  --with_pbt WITH_PBT   Enables population-based training (PBT) (default:\n                        False)\n  --pbt_mix_policies_in_one_env PBT_MIX_POLICIES_IN_ONE_ENV\n                        For multi-agent envs, whether we mix different\n                        policies in one env. (default: True)\n  --pbt_period_env_steps PBT_PERIOD_ENV_STEPS\n                        Periodically replace the worst policies with the best\n                        ones and perturb the hyperparameters (default:\n                        5000000)\n  --pbt_start_mutation PBT_START_MUTATION\n                        Allow initial diversification, start PBT after this\n                        many env steps (default: 20000000)\n  --pbt_replace_fraction PBT_REPLACE_FRACTION\n                        A portion of policies performing worst to be replace\n                        by better policies (rounded up) (default: 0.3)\n  --pbt_mutation_rate PBT_MUTATION_RATE\n                        Probability that a parameter mutates (default: 0.15)\n  --pbt_replace_reward_gap PBT_REPLACE_REWARD_GAP\n                        Relative gap in true reward when replacing weights of\n                        the policy with a better performing one (default: 0.1)\n  --pbt_replace_reward_gap_absolute PBT_REPLACE_REWARD_GAP_ABSOLUTE\n                        Absolute gap in true reward when replacing weights of\n                        the policy with a better performing one (default:\n                        1e-06)\n  --pbt_optimize_gamma PBT_OPTIMIZE_GAMMA\n                        Whether to optimize gamma, discount factor, or not\n                        (experimental) (default: False)\n  --pbt_target_objective PBT_TARGET_OBJECTIVE\n                        Policy stat to optimize with PBT. true_objective\n                        (default) is equal to raw env reward if not specified,\n                        but can also be any other per-policy stat.For DMlab-30\n                        use value \"dmlab_target_objective\" (which is capped\n                        human normalized score) (default: true_objective)\n  --pbt_perturb_min PBT_PERTURB_MIN\n                        When PBT mutates a float hyperparam, it samples the\n                        change magnitude randomly from the uniform\n                        distribution [pbt_perturb_min, pbt_perturb_max]\n                        (default: 1.1)\n  --pbt_perturb_max PBT_PERTURB_MAX\n                        When PBT mutates a float hyperparam, it samples the\n                        change magnitude randomly from the uniform\n                        distribution [pbt_perturb_min, pbt_perturb_max]\n                        (default: 1.5)\n</code></pre>"},{"location":"02-configuration/configuration/","title":"Configuration","text":"<p>Sample Factory experiments are configured via command line parameters. The following command will print the help message for the algorithm-environment combination containing the list of all parameters, their descriptions, and their default values:</p> <pre><code>python -m sf_examples.train_gym_env --env=CartPole-v1 --help\n</code></pre> <p>(replace <code>train_gym_env</code> with your own training script name and <code>CartPole-v1</code> with a different environment name to get information about parameters specific to this particular environment).</p> <p>Default parameter values and their help strings are defined in <code>sample_factory/cfg/cfg.py</code>. Besides that, additional parameters can be defined in specific environment integrations, for example in <code>sf_examples/envpool/mujoco/envpool_mujoco_params.py</code>.</p>"},{"location":"02-configuration/configuration/#configjson","title":"config.json","text":"<p>Once the new experiment is started, a directory containing experiment-related files is created in <code>--train_dir</code> location (or <code>./train_dir</code> in <code>cwd</code> if <code>--train_dir</code> is not passed from command line). This directory contains a file <code>config.json</code> where all the experiment parameters are saved (including those instantiated from their default values).</p> <p>In addition to that, selected parameter values are printed to the console and thus are saved to <code>sf_log.txt</code> file in the experiment directory. Running an experiment and then stopping it to check the parameter values is a good practice to make sure that the experiment is configured as expected.</p>"},{"location":"02-configuration/configuration/#key-parameters","title":"Key parameters","text":"<ul> <li> <p><code>--env</code> (required) full name that uniquely identifies the environment as it is registered in the environment registry (see <code>register_env()</code> function).</p> </li> <li> <p><code>--experiment</code> a name that uniquely identifies the experiment and the experiment folder. E.g. <code>--experiment=my_experiment</code>. If the experiment folder with the name already exists the experiment (by default) will be resumed! Resuming experiments after a stop is the default behavior in Sample Factory.  When the experiment is resumed from command line are taken into account, unspecified parameters will be loaded from the existing experiment <code>config.json</code> file. If you want to start a new experiment, delete the old experiment folder or change the experiment name. You can also use <code>--restart_behavior=[resume|restart|overwrite]</code> to control this behavior.</p> </li> <li> <p><code>--train_dir</code> location for all experiments folders, defaults to <code>./train_dir</code>.</p> </li> <li> <p><code>--num_workers</code> defaults to number of logical cores in the system, which will give the best throughput in most scenarios.</p> </li> <li> <p><code>--num_envs_per_worker</code> will greatly affect the performance. Large values (15-30) improve hardware utilization but increase memory usage and policy lag. Must be even for the double-buffered sampling to work. Disable double-buffered sampling by setting <code>--worker_num_splits=1</code> to use odd number of envs per worker (e.g. 1 env per worker). (Default: 2) A good rule of thumb is to set this to relatively low value (e.g. 4 or 8 for common envs) and then increase it until you see no more performance improvements or you start losing sample efficiency due to the policy lag.</p> </li> <li> <p><code>--rollout</code> is the length of trajectory collected by each agent.</p> </li> <li> <p><code>--batch_size</code> is the minibatch size for SGD.</p> </li> <li><code>--num_batches_per_epoch</code> is the number of minibatches the training batch (dataset) is split into.</li> <li><code>--num_epochs</code> is the number of epochs on the learner over one training batch (dataset).</li> </ul> <p>The above six parameters (<code>batch_size, num_batches_per_epoch, rollout, num_epochs, num_workers, num_envs_per_worker</code>) have the biggest influence on the data regime of the RL algorithm and thus on the sample efficiency and the training speed.</p> <p><code>num_workers</code>, <code>num_envs_per_worker</code>, and <code>rollout</code> define how many samples are collected per iteration (one rollout for all envs), which is <code>sampling_size = num_workers * num_envs_per_worker * rollout</code> (note that this is further multiplied by env's <code>num_agents</code> for multi-agent envs).</p> <p><code>batch_size</code> and <code>num_batches_per_epoch</code> define how many samples are used for training per iteration.</p> <p>If <code>sampling_size &gt;&gt; batch_size</code> then we will need many iterations of training to go through the data, which will make some experience stale by the time it is used for training (policy lag). See Policy Lag for additional information.</p>"},{"location":"02-configuration/configuration/#evaluation-script-parameters","title":"Evaluation script parameters","text":"<p>Evaluation scripts (i.e. <code>sf_examples/atari/enjoy_atari.py</code>) use the same configuration parameters as training scripts for simplicity, although of course many of them are ignored as they don't affect evaluation.</p> <p>In addition to that, evaluation scripts provide additional parameters, see <code>add_eval_args()</code> in <code>sample_factory/cfg/cfg.py</code>. HuggingFace Hub integration guide provides a good overview of the important parameters such as <code>--save_video</code>, check it out!</p>"},{"location":"02-configuration/configuration/#full-list-of-parameters","title":"Full list of parameters","text":"<p>Please see the Full Parameter Reference auto-generated using the <code>--help</code> flag for the full list of available command line arguments.</p>"},{"location":"03-customization/custom-environments/","title":"Custom environments","text":"<p>Training agents in your own environment with Sample Factory is straightforward, but if you get stuck feel free to raise an issue on our GitHub Page.</p> <p>We recommend looking at our example environment integrations such as Atari or MuJoCo before using your own environment.</p>"},{"location":"03-customization/custom-environments/#custom-environment-template","title":"Custom environment template","text":"<p>In order to integrate your own environment with Sample Factory, the following steps are required:</p> <ul> <li>Define entry points for training and evaluation scripts, such as <code>train_custom_env.py</code> and <code>enjoy_custom_env.py</code>.</li> <li>Define a method that creates an instance of your environment, such as <code>make_custom_env()</code>.</li> <li>Override any default parameters that are specific to your environment, this way you can avoid passing them from the command line (optional).</li> <li>Add any custom parameters that will be parsed by Sample Factory alongside the default parameters (optional).</li> </ul> <p>We provide the following template, which you can modify to intergrate your environment. We assume your environment conforms to the gym 0.26 API (5-tuple).</p> <pre><code>from typing import Optional\nimport argparse\nimport sys\n\nfrom sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args\nfrom sample_factory.envs.env_utils import register_env\nfrom sample_factory.train import run_rl\n\n\ndef make_custom_env(full_env_name: str, cfg=None, env_config=None, render_mode: Optional[str] = None):\n    # see the section below explaining arguments\n    return CustomEnv(full_env_name, cfg, env_config, render_mode=render_mode)\n\ndef register_custom_env_envs():\n    # register the env in sample-factory's global env registry\n    # after this, you can use the env in the command line using --env=custom_env_name\n    register_env(\"custom_env_name\", make_custom_env)\n\ndef add_custom_env_args(_env, p: argparse.ArgumentParser, evaluation=False):\n    # You can extend the command line arguments here\n    p.add_argument(\"--custom_argument\", default=\"value\", type=str, help=\"\")\n\ndef custom_env_override_defaults(_env, parser):\n    # Modify the default arguments when using this env.\n    # These can still be changed from the command line. See configuration guide for more details.\n    parser.set_defaults(\n        encoder_conv_architecture=\"convnet_atari\",\n        obs_scale=255.0,\n        gamma=0.99,\n        learning_rate=0.00025,\n        lr_schedule=\"linear_decay\",\n        adam_eps=1e-5,  \n    )\n\ndef parse_args(argv=None, evaluation=False):\n    # parse the command line arguments to build\n    parser, partial_cfg = parse_sf_args(argv=argv, evaluation=evaluation)\n    add_custom_env_args(partial_cfg.env, parser, evaluation=evaluation)\n    custom_env_override_defaults(partial_cfg.env, parser)\n    final_cfg = parse_full_cfg(parser, argv)\n    return final_cfg\n\ndef main():\n    \"\"\"Script entry point.\"\"\"\n    register_custom_env_envs()\n    cfg = parse_args()\n\n    status = run_rl(cfg)\n    return status\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <p>Training can now be started with <code>python train_custom_env.py --env=custom_env_name --experiment=CustomEnv</code>. Note that this train script can be defined in your own codebase, or in the Sample Factory codebase (in case you forked the repo).</p>"},{"location":"03-customization/custom-environments/#environment-factory-function-parameters","title":"Environment factory function parameters","text":"<p><code>register_env(\"custom_env_name\", make_custom_env)</code> expects <code>make_custom_env</code> to be a Callable with the following signature:</p> <pre><code>def make_custom_env_func(full_env_name: str, cfg: Optional[Config] = None, env_config: Optional[AttrDict] = None, render_mode: Optional[str] = None) -&gt; Env\n</code></pre> <p>Arguments: * <code>full_env_name</code>: complete name of the environment as passed in the command line with <code>--env</code> * <code>cfg</code>: full system configuration, output of argparser. Normally this is an <code>AttrDict</code> (dictionary where keys can be accessed as attributes) * <code>env_config</code>: AttrDict with additional system information, for example: <code>env_config = AttrDict(worker_index=worker_idx, vector_index=vector_idx, env_id=env_id)</code> Some custom environments will require this information, i.e. <code>env_id</code> is a unique identifier for each environment instance in 0..num_envs-1 range.  * <code>render_mode</code>: if not None, environment will be rendered in this mode (e.g. 'human', 'rgb_array'). New parameter required after Gym 0.26.</p> <p>See <code>sample_factory/envs/create_env.py</code> for more details.</p>"},{"location":"03-customization/custom-environments/#evaluation-script-template","title":"Evaluation script template","text":"<p>The evaluation script template is even more straightforward. Note that we just reuse functions already defined in the training script.</p> <pre><code>import sys\n\nfrom sample_factory.enjoy import enjoy\nfrom train_custom_env import parse_args, register_custom_env_envs\n\n\ndef main():\n    \"\"\"Script entry point.\"\"\"\n    register_custom_env_envs()\n    cfg = parse_args(evaluation=True)\n    status = enjoy(cfg)\n    return status\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <p>You can now run evaluation with <code>python enjoy_custom_env.py --env=custom_env_name --experiment=CustomEnv</code> to measure the performance of the trained model, visualize agent's performance, or record a video file.</p>"},{"location":"03-customization/custom-environments/#examples","title":"Examples","text":"<ul> <li><code>sf_examples/train_custom_env_custom_model.py</code> - integrates an entirely custom toy environment.</li> <li><code>sf_examples/train_gym_env.py</code> - trains an agent in a Gym environment. Environments registered in <code>gym</code> do not get any special treatment, as it is just another way to define an environment. In this case the environment creation function reduces to <code>gym.make(env_name)</code>.</li> <li>See environment integrations in <code>sf_examples/&lt;env_name&gt;</code> for additional examples.</li> </ul>"},{"location":"03-customization/custom-models/","title":"Custom models","text":"<p>Adding custom models in Sample Factory is simple, but if you get stuck feel free to raise an issue on our GitHub Page.</p>"},{"location":"03-customization/custom-models/#actor-critic-models-in-sample-factory","title":"Actor Critic models in Sample Factory","text":"<p>Actor Critic models in Sample Factory are composed of three components:</p> <ul> <li>Encoder - Process input observations (images, vectors) and map them to a vector. This is the part of the model you will most likely want to customize.</li> <li>Core - Intergrate vectors from one or more encoders, can optionally include a single- or multi-layer LSTM/GRU in a memory-based agent.</li> <li>Decoder - Apply additional layers to the output of the model core before computing the policy and value outputs.</li> </ul> <p>Regardless of the component customization, you can use your resulting model in \"shared weights\" or \"separate weights\" regime (either sharing or not sharing the weights between the policy and value networks). This is controlled by the <code>--actor_critic_share_weights=[True|False]</code> command line argument.</p> <p>On top of that, you can register an entire custom Actor Critic model. This can be useful for more complex models,  for example centralized critic for multi-agent envs, asymmetric actor-critic where critic observes more information, which can be useful in sim-to-real, and so on.</p>"},{"location":"03-customization/custom-models/#custom-model-template","title":"Custom model template","text":"<p>The following template demonstrates how different components of the model can be customized. Feel free to combine this with the custom environment template above to create a fully custom environment &amp; model combination.</p> <pre><code>from sample_factory.model.encoder import Encoder\nfrom sample_factory.model.decoder import Decoder\nfrom sample_factory.model.core import ModelCore\nfrom sample_factory.model.actor_critic import ActorCritic\nfrom sample_factory.algo.utils.context import global_model_factory\n\n\nclass CustomEncoder(Encoder):\n    def __init__(self, cfg: Config, obs_space: ObsSpace):\n        super().__init__(cfg)\n        # build custom encoder architecture\n        ...\n\n    def forward(self, obs_dict):\n        # custom forward logic\n        ...\n\nclass CustomCore(ModelCore):\n    def __init__(self, cfg: Config, input_size: int):\n        super().__init__(cfg)\n        # build custom core architecture\n        ...\n\n    def forward(self, head_output, rnn_states):\n        # custom forward logic\n        ...\n\n\nclass CustomDecoder(Decoder):\n    def __init__(self, cfg: Config, decoder_input_size: int):\n        super().__init__(cfg)\n        # build custom decoder architecture\n        ...\n\n    def forward(self, core_output):\n        # custom forward logic\n        ...\n\nclass CustomActorCritic(ActorCritic):\n    def __init__(\n        self,\n        model_factory,\n        obs_space: ObsSpace,\n        action_space: ActionSpace,\n        cfg: Config,\n    ):\n    super().__init__(obs_space, action_space, cfg)\n\n    self.encoder = CustomEncoder(cfg, obs_space)\n    self.core = CustomCore(cfg, self.encoder.get_out_size())\n    self.decoder = CustomDecoder(cfg, self.core.get_out_size())\n    self.critic_linear = nn.Linear(self.decoder.get_out_size())\n    self.action_parameterization = self.get_action_parameterization(\n        self.decoder.get_out_size()\n    ) \n\n    def forward(self, normalized_obs_dict, rnn_states, values_only=False):\n        # forward logic\n        ...\n\n\ndef register_model_components():\n    # register custom components with the factory\n    # you can register an entire Actor Critic model\n    global_model_factory().register_actor_critic_factory(CustomActorCritic)\n\n    # or individual components\n    global_model_factory().register_encoder_factory(CustomEncoder)\n    global_model_factory().register_model_core_factory(CustomCore)\n    global_model_factory().register_decoder_factory(CustomDecoder)\n\ndef main():\n    \"\"\"Script entry point.\"\"\"\n    register_model_components()\n    cfg = parse_args()\n\n    status = run_rl(cfg)\n    return status\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre>"},{"location":"03-customization/custom-models/#examples","title":"Examples","text":"<p>Examples of model customizations can be found in:</p> <ul> <li><code>sf_examples/train_custom_env_custom_model.py</code></li> <li><code>sf_examples/isaacgym_examples/train_isaacgym.py</code></li> <li><code>sf_examples/dmlab/dmlab_model.py</code></li> <li><code>sf_examples/vizdoom/doom/doom_model.py</code></li> </ul>"},{"location":"03-customization/custom-multi-agent-environments/","title":"Custom multi-agent environments","text":"<p>Multi-agent environments are expected to return lists (or tuples, arrays, tensors) of observations/rewards/etc, one item for every agent.</p> <p>It is expected that a multi-agent env exposes a property or a member variable <code>num_agents</code> that the algorithm uses to allocate the right amount of memory during startup.</p> <p>Multi-agent environments require auto-reset! I.e. they reset a particular agent when the corresponding <code>terminated</code> or <code>truncated</code> flag is <code>True</code> and return  the first observation of the next episode (because we have no use for the last observation of the previous episode, we do not act based on it).</p> <p>For simplicity Sample Factory actually treats all environments as multi-agent, i.e. single-agent environments are automatically treated as multi-agent environments with one agent with the use of a wrapper.</p> <p>In rare cases we may deal with an environment that should not be additionally wrapped, i.e. a single-agent version of a multi-agent env may already return lists of length 1. In this case, your environment should define a member variable <code>is_multiagent=True</code>, and Sample Factory will not wrap it.</p>"},{"location":"03-customization/custom-multi-agent-environments/#examples","title":"Examples","text":"<ul> <li><code>sf_examples/enjoy_custom_multi_env.py</code> - integrates and entirely custom toy example multi-agent env. Use this as a template for your own multi-agent env.</li> <li><code>sf_examples/isaacgym_examples/train_isaacgym.py</code> - technically IsaacGym is not a multi-agent environment because different agents don't interact.  It is a vectorized environment simulating many agents with a single env instance, but is treated as a multi-agent environment by Sample Factory.</li> <li><code>sf_examples/vizdoom/doom/multiplayer</code> - this is a rather advanced example, here we connect multiple VizDoom instances into a single multi-agent match and expose a multi-agent env interface to Sample Factory. </li> </ul>"},{"location":"03-customization/custom-multi-agent-environments/#further-reading","title":"Further reading","text":"<ul> <li>Multi-agent environments can be combined with multi-policy training and Population Based Training (PBT).</li> <li>Sometimes it makes sense to disable some of the agents in a multi-agent environment. For example, in a multi-player game some agents might die in the middle of the episode and should not contribute any rollouts until the episode reset. This can be achieved using inactive agents feature.</li> </ul>"},{"location":"04-experiments/experiment-launcher/","title":"Experiment Launcher","text":"<p>The simplest way to run experiments is just through command line, see Basic Usage for example.</p> <p>For more complex workflows Sample Factory provides an interface that allows users to run experiments with multiple seeds or hyperparameter combinations with automatic distribution of work across GPUs on a single machine or multiple machines on the cluster.</p> <p>The configuration of such experiments is done through in Python code, i.e. instead of yaml or json files we directly use Python scripts for ultimate flexibility.</p>"},{"location":"04-experiments/experiment-launcher/#launcher-scripts","title":"Launcher scripts","text":"<p>Take a look at <code>sf_examples/mujoco/experiments/mujoco_all_envs.py</code>:</p> <pre><code>from sample_factory.launcher.run_description import Experiment, ParamGrid, RunDescription\n\n_params = ParamGrid(\n    [\n        (\"seed\", [0, 1111, 2222, 3333, 4444, 5555, 6666, 7777, 8888, 9999]),\n        (\"env\", [\"mujoco_ant\", \"mujoco_halfcheetah\", \"mujoco_hopper\", \"mujoco_humanoid\", \"mujoco_doublependulum\", \"mujoco_pendulum\", \"mujoco_reacher\", \"mujoco_swimmer\", \"mujoco_walker\"]),\n    ]\n)\n\n_experiments = [\n    Experiment(\n        \"mujoco_all_envs\",\n        \"python -m sf_examples.mujoco.train_mujoco --algo=APPO --with_wandb=True --wandb_tags mujoco\",\n        _params.generate_params(randomize=False),\n    ),\n]\n\nRUN_DESCRIPTION = RunDescription(\"mujoco_all_envs\", experiments=_experiments)\n</code></pre> <p>This script defines a list of experiments to run. Here we have 10 seeds and 9 environments, so we will run 90 experiments in total with 90 different seed/env combinations. This can be extended in a straightforward way to run hyperparameter searches and so on.</p> <p>The only requirement for such a script is that it defines a <code>RUN_DESCRIPTION</code> variable that references a <code>RunDescription</code> object. This object contains a list of <code>Experiment</code> objects, each of which potentially defines a gridsearch to run. Each experiment object defines a name, a \"base\" command line to run, and a <code>ParamGrid</code> that will generate parameter combinations to be added to the base command line. Take a look at other experiment scripts in <code>sf_examples</code> to see how to define more complex experiments.</p> <p>Note that there's no requirement to use Launcher API to run experiments. You can just run individual experiments from the command line, use WandB hyperparam search features, use Ray Tune or any other tool you like. Launcher API is just a convenient feature for simple workflows available out of the box.</p>"},{"location":"04-experiments/experiment-launcher/#complex-hyperparameter-configurations","title":"Complex hyperparameter configurations","text":"<p>The <code>ParamGrid</code> object above can define a cartesian product of parameter lists. In some cases we want searches over pairs (or tuples) of parameters at the same time.</p> <p>For example:</p> <pre><code>_params = ParamGrid(\n    [\n        (\"seed\", [1111, 2222, 3333, 4444]),\n        ((\"serial_mode\", \"async_rl\"), ([True, False], [False, True])),\n        ((\"use_rnn\", \"recurrence\"), ([False, 1], [True, 16])),\n    ]\n)\n</code></pre> <p>Here we consider parameter pairs <code>(\"serial_mode\", \"async_rl\")</code> and <code>(\"use_rnn\", \"recurrence\")</code> at the same time. If we used a simple grid, we would have to execute useless combinations of parameters such as <code>use_rnn=True, recurrence=1</code> or <code>use_rnn=False, recurrence=16</code> (it makes sense to use recurrence &gt; 1 only when using RNNs).</p>"},{"location":"04-experiments/experiment-launcher/#rundescription-arguments","title":"RunDescription arguments","text":"<p>Launcher script should expose a RunDescription object named <code>RUN_DESCRIPTION</code> that contains a list of experiments to run and some auxiliary parameters. <code>RunDescription</code> parameter reference:</p> <pre><code>class RunDescription:\n    def __init__(\n        self,\n        run_name,\n        experiments,\n        experiment_arg_name=\"--experiment\",\n        experiment_dir_arg_name=\"--train_dir\",\n        customize_experiment_name=True,\n        param_prefix=\"--\",\n    ):\n        \"\"\"\n        :param run_name: overall name of the experiment and the name of the root folder\n        :param experiments: a list of Experiment objects to run\n        :param experiment_arg_name: CLI argument of the underlying experiment that determines it's unique name\n               to be generated by the launcher. Default: --experiment\n        :param experiment_dir_arg_name: CLI argument for the root train dir of your experiment. Default: --train_dir\n        :param customize_experiment_name: whether to add a hyperparameter combination to the experiment name\n        :param param_prefix: most experiments will use \"--\" prefix for each parameter, but some apps don't have this\n               prefix, i.e. with Hydra you should set it to empty string.\n        \"\"\"\n</code></pre>"},{"location":"04-experiments/experiment-launcher/#using-a-launcher-script","title":"Using a launcher script","text":"<p>The script above can be executed using one of several backends. Additional backends are a welcome contribution! Please submit PRs :)</p>"},{"location":"04-experiments/experiment-launcher/#local-backend-multiprocessing","title":"\"Local\" backend (multiprocessing)","text":"<p>Command line below will run all experiments on a single 4-GPU machine, scheduling 2 experiments per GPU, so running 8 experiments in parallel until all 90 are done. Note how we pass the full path to the launcher script using <code>--run</code> argument. The script should be in your Python path in a way that you should be able to import the module using the path you pass to <code>--run</code> (because this is what the Launcher internally does).</p> <pre><code>python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=processes --max_parallel=8  --pause_between=1 --experiments_per_gpu=2 --num_gpus=4\n</code></pre>"},{"location":"04-experiments/experiment-launcher/#slurm-backend","title":"Slurm backend","text":"<p>The following command will run experiments on a Slurm cluster, creating a separate job for each experiment.</p> <pre><code>python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=slurm --slurm_workdir=./slurm_isaacgym --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False\n</code></pre> <p>Here we will use 1 GPU and 16 CPUs per job (adjust according to your cluster configuration and experiment config). Note how we also pass <code>--slurm_sbatch_template</code> argument which contains a bash script that will bootstrap a job.  In this particular example we use a template that will kill the job if it runs longer than a certain amount of time and then restarts itself  (controlled by <code>--slurm_timeout</code> which defaults to 0, i.e. no timeout). Feel free to use your custom template if your job has certain pre-requisites (i.e. installing some packages or activating a Python environment).</p> <p>Please find additional Slurm considerations in How to use Sample Factory on Slurm guide.</p>"},{"location":"04-experiments/experiment-launcher/#ngc-backend","title":"NGC backend","text":"<p>We additionally provide a backend for NGC clusters (https://ngc.nvidia.com/).</p> <pre><code>python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=ngc --ngc_job_template=run_scripts/ngc_job_16g_1gpu.template --ngc_print_only=False --train_dir=/workspace/train_dir\n</code></pre> <p>Here <code>--ngc_job_template</code> contains information about which Docker image to run plus any additional job bootstrapping. The command will essentially spin a separate VM on the cloud for each job. Point <code>--train_dir</code> to a mounted workspace folder so that you can access results of your experiments (trained models, logs, etc.)</p>"},{"location":"04-experiments/experiment-launcher/#additional-cli-examples","title":"Additional CLI examples","text":"<pre><code>Local multiprocessing backend:\n$ python -m sample_factory.launcher.run --run=sf_examples.vizdoom.experiments.paper_doom_battle2_appo --backend=processes --max_parallel=4 --pause_between=10 --experiments_per_gpu=1 --num_gpus=4\n\nParallelize with Slurm:\n$ python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./megaverse_single_agent --experiment_suffix=slurm --pause_between=1 --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=12 --slurm_sbatch_template=./megaverse_rl/slurm/sbatch_template.sh --slurm_print_only=False\n\nParallelize with NGC (https://ngc.nvidia.com/):\n$ python -m sample_factory.launcher.run --run=rlgpu.run_scripts.dexterous_manipulation --backend=ngc --ngc_job_template=run_scripts/ngc_job_16g_1gpu.template --ngc_print_only=False --train_dir=/workspace/train_dir\n</code></pre>"},{"location":"04-experiments/experiment-launcher/#command-line-reference","title":"Command-line reference","text":"<pre><code>usage: run.py [-h] [--train_dir TRAIN_DIR] [--run RUN]\n              [--backend {processes,slurm,ngc}]\n              [--pause_between PAUSE_BETWEEN]\n              [--experiment_suffix EXPERIMENT_SUFFIX]\n\n# Multiprocessing backend:\n              [--num_gpus NUM_GPUS]\n              [--experiments_per_gpu EXPERIMENTS_PER_GPU]\n              [--max_parallel MAX_PARALLEL]\n\n# Slurm-related:\n              [--slurm_gpus_per_job SLURM_GPUS_PER_JOB]\n              [--slurm_cpus_per_gpu SLURM_CPUS_PER_GPU]\n              [--slurm_print_only SLURM_PRINT_ONLY]\n              [--slurm_workdir SLURM_WORKDIR]\n              [--slurm_partition SLURM_PARTITION]\n              [--slurm_sbatch_template SLURM_SBATCH_TEMPLATE]\n\n# NGC-related\n              [--ngc_job_template NGC_JOB_TEMPLATE]\n              [--ngc_print_only NGC_PRINT_ONLY]\n</code></pre> <pre><code>Arguments:\n  -h, --help            show this help message and exit\n  --train_dir TRAIN_DIR\n                        Directory for sub-experiments\n  --run RUN             Name of the python module that describes the run, e.g.\n                        sf_examples.vizdoom.experiments.doom_basic\n  --backend {processes,slurm,ngc}\n  --pause_between PAUSE_BETWEEN\n                        Pause in seconds between processes\n  --experiment_suffix EXPERIMENT_SUFFIX\n                        Append this to the name of the experiment dir\n\nMultiprocessing backend:\n  --num_gpus NUM_GPUS   How many GPUs to use (only for local multiprocessing)\n  --experiments_per_gpu EXPERIMENTS_PER_GPU\n                        How many experiments can we squeeze on a single GPU\n                        (-1 for not altering CUDA_VISIBLE_DEVICES at all)\n  --max_parallel MAX_PARALLEL\n                        Maximum simultaneous experiments (only for local multiprocessing)\n\nSlurm-related:\n  --slurm_gpus_per_job SLURM_GPUS_PER_JOB\n                        GPUs in a single SLURM process\n  --slurm_cpus_per_gpu SLURM_CPUS_PER_GPU\n                        Max allowed number of CPU cores per allocated GPU\n  --slurm_print_only SLURM_PRINT_ONLY\n                        Just print commands to the console without executing\n  --slurm_workdir SLURM_WORKDIR\n                        Optional workdir. Used by slurm launcher to store\n                        logfiles etc.\n  --slurm_partition SLURM_PARTITION\n                        Adds slurm partition, i.e. for \"gpu\" it will add \"-p\n                        gpu\" to sbatch command line\n  --slurm_sbatch_template SLURM_SBATCH_TEMPLATE\n                        Commands to run before the actual experiment (i.e.\n                        activate conda env, etc.) Example: https://github.com/alex-petrenko/megaverse/blob/master/megaverse_rl/slurm/sbatch_template.sh\n                        (typically a shell script)\n  --slurm_timeout SLURM_TIMEOUT\n                        Time to run jobs before timing out job and requeuing the job. Defaults to 0, which does not time out the job\n\nNGC-related:\n  --ngc_job_template NGC_JOB_TEMPLATE\n                        NGC command line template, specifying instance type, docker container, etc.\n  --ngc_print_only NGC_PRINT_ONLY\n                        Just print commands to the console without executing\n</code></pre>"},{"location":"04-experiments/slurm-details/","title":"Sample Factory on Slurm","text":"<p>This section contains instructions for running Sample Factory experiments using Slurm.</p>"},{"location":"04-experiments/slurm-details/#setting-up","title":"Setting up","text":"<p>Login to your Slurm login node using ssh with your username and password. Start an interactive job with <code>srun</code> to install files to your NFS. </p> <pre><code>srun -c40 --gres=gpu:1 --pty bash\n</code></pre> <p>Note that you may get a message <code>groups: cannot find name for group ID XXXX</code> which is not an error.</p> <p>Install Miniconda:</p> <ul> <li>Download installer using <code>wget</code> from https://docs.conda.io/en/latest/miniconda.html#linux-installers</li> <li>Run the installer with <code>bash {Miniconda...sh}</code></li> </ul> <p>Make new conda environment <code>conda create --name sf2</code> then <code>conda activate sf2</code></p> <p>Download Sample Factory and install dependencies, for example:</p> <pre><code>git clone https://github.com/alex-petrenko/sample-factory.git\ncd sample-factory\ngit checkout sf2\npip install -e .\n# install additional env dependencies here if needed\n</code></pre>"},{"location":"04-experiments/slurm-details/#necessary-scripts-in-sample-factory","title":"Necessary scripts in Sample Factory","text":"<p>To run a custom launcher script for Sample Factory on slurm, you may need to write your own slurm_sbatch_template and/or launcher script.</p> <p>slurm_sbatch_template is a bash script that run by slurm before your python script. It includes commands to activate your conda environment etc. See an example at <code>./sample_factory/launcher/slurm/sbatch_timeout.sh</code>. Variables in the bash script can be added in <code>sample_factory.launcher.run_slurm</code>.</p> <p>The launcher script controls the Python command slurm will run. Examples are located in <code>sf_examples</code>. You can run multiple experiments with different parameters using <code>ParamGrid</code>.</p>"},{"location":"04-experiments/slurm-details/#timeout-script","title":"Timeout script","text":"<p>If your slurm cluster has time limits for jobs, you can use the <code>sbatch_timeout.sh</code> bash script to launch jobs that timeout and requeue themselves before the time limit. </p> <p>The time limit can be set with the <code>--slurm_timeout</code> command line argument. It defaults to <code>0</code> which runs the job with no time limit. It is recommended the timeout be set to slightly less than the time limit of your job. For example, if the time limit is 24 hours, you should set <code>--slurm_timeout=23h</code></p>"},{"location":"04-experiments/slurm-details/#running-launcher-scripts-on-slurm","title":"Running launcher scripts on Slurm","text":"<p>Activate your conda environment <code>conda activate sf2</code> then <code>cd sample-factory</code></p> <p>Run your launcher script - an example mujoco launcher (replace run, slurm_sbatch_template, and slurm_workdir with appropriate values) <pre><code>python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=slurm --slurm_workdir=./slurm_mujoco --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False \n</code></pre></p> <p>The <code>slurm_gpus_per_job</code> and <code>slurm_cpus_per_gpu</code> determine the resources allocated to each job. You can view the jobs without running them by setting <code>slurm_print_only=True</code>.</p> <p>You can view the status of your jobs on nodes or the queue with <code>squeue</code> and view the outputs of your experiments with <code>tail -f {slurm_workdir}/*.out</code>. Cancel your jobs with <code>scancel {job_id}</code></p>"},{"location":"05-monitoring/custom-metrics/","title":"Custom Summaries","text":""},{"location":"05-monitoring/custom-metrics/#environment-specific-info","title":"Environment-specific info","text":"<p>It is often useful to monitor custom training metrics, i.e. certain environment-specific aspects of agent's performance.</p> <p>You can add custom monitored metrics by adding <code>info[\"episode_extra_stats\"] = { ... }</code> to the environment's <code>info</code> dictionary returned from the <code>step()</code> function on the last step of the episode.</p> <p>See <code>sf_examples/dmlab/wrappers/reward_shaping.py</code> for example. Here we add information about  agent's performance on individual levels in DMLab-30.</p>"},{"location":"05-monitoring/custom-metrics/#custom-metrics","title":"Custom metrics","text":"<p>You can add completely custom metrics that are calculated based on other metrics or the RL algorithm state. To do this, add a custom algo observer that overrides <code>extra_summaries()</code> function.</p> <p>See <code>sf_examples/dmlab/train_dmlab.py</code> where we define <code>DmlabExtraSummariesObserver</code> that aggregates custom  environment metrics to produce a single \"Human-normalized score\" summary.</p>"},{"location":"05-monitoring/metrics-reference/","title":"Metrics Reference","text":""},{"location":"05-monitoring/metrics-reference/#general-information","title":"General information","text":"<p>Each experiment will have at least the following groups of metrics on Tensorboard/Wandb:</p> <ul> <li><code>len</code></li> <li><code>perf</code></li> <li><code>policy_stats</code></li> <li><code>reward</code></li> <li><code>stats</code></li> <li><code>train</code></li> </ul> <p>Plus new sections (groups) are created for each custom metric with key in <code>&lt;group_name&gt;/&lt;metric_name&gt;</code> format (see Custom Metrics section).</p> <p>Summaries such as <code>len</code>, <code>perf</code>, <code>reward</code> are averaged over the last 100 data points to filter noise (this can be changed by <code>--stats_avg=N</code> argument). These summaries are written to Tensorboard/Wandb every <code>--experiment_summaries_interval</code> seconds (10 seconds by default).</p> <p><code>train</code> summaries are not averaged and just represent the values from the latest minibatch on the learner. The reporting rate for <code>train</code> summaries is decayed over time to reduce the size of the log files. The schedule is controlled by <code>summary_rate_decay_seconds</code> variable in <code>learner.py</code>.</p>"},{"location":"05-monitoring/metrics-reference/#len","title":"<code>len</code>","text":"<p><code>len/len</code>, <code>len/len_max</code>, <code>len/len_min</code> are simply episode lengths measured after frameskip. If your environment uses frameskip=4 and the reported episode length is 400, it means that 400 environment steps were simulated but the agent actually observed only 100 frames.</p>"},{"location":"05-monitoring/metrics-reference/#perf","title":"<code>perf</code>","text":"<p><code>perf/_fps</code> and <code>perf/_sample_throughput</code> represent throughput as measured in different parts of the algorithm.</p> <p><code>perf/_sample_throughput</code> is the number of observations processed (or actions generated) by the inference worker, i.e. pure  sampling throughput measured before frameskipping is taken into account.</p> <p><code>perf/_fps</code> is the number of observations/actions processed by the learner and measured after frameskipping.</p> <p>For example with frameskip=4, <code>perf/_sample_throughput</code> will be 4 times smaller than <code>perf/_fps</code>. If this is not the case, it means that the learner had to throw away some trajectories which can happen for multiple reasons, for example if the trajectories were too stale and exceeded <code>--max_policy_lag</code>.</p>"},{"location":"05-monitoring/metrics-reference/#policy_stats","title":"<code>policy_stats</code>","text":"<p>By default this section only contains the <code>true_objective</code> metrics: <code>policy_stats/avg_true_objective</code>,  <code>policy_stats/avg_true_objective_max</code>, <code>policy_stats/avg_true_objective_min</code>. This will reflect the <code>true_objective</code> value if the environment returns one in the <code>info</code> dictionary (see PBT for more details).</p> <p>If <code>true_objective</code> is not specified these metrics should be equal to the scalar environment reward.</p> <p><code>policy_stats</code> will also contain any custom metrics (see Custom metrics) that are not in  <code>&lt;group_name&gt;/&lt;metric_name&gt;</code> format.</p>"},{"location":"05-monitoring/metrics-reference/#reward","title":"<code>reward</code>","text":"<p><code>reward/reward</code>, <code>reward/reward_max</code>, <code>reward/reward_min</code> are the raw scalar environment rewards, reported before any scaling (<code>--reward_scale</code>) or normalization is applied.</p>"},{"location":"05-monitoring/metrics-reference/#stats","title":"<code>stats</code>","text":"<ul> <li><code>stats/avg_request_count</code> - how many requests from the rollout workers are processed per inference step. The correpondence between this number and the actual inference batch size depends on training configuration, this is mostly an internal metric for debugging purposes.</li> <li><code>stats/gpu_cache_learner</code>, <code>stats/gpu_cache_policy_worker</code>, <code>stats/gpu_mem_learner</code>, <code>stats/gpu_mem_policy_worker</code>, <code>stats/gpu_mem_policy_worker</code>, <code>stats/master_process_memory_mb</code>, <code>stats/memory_learner</code>, <code>stats/memory_policy_worker</code> -  a group of metrics to keep track of RAM and VRAM usage, mostly used to detect and debug memory leaks.</li> <li><code>stats/step_policy</code>, <code>stats/wait_policy</code> - performance debugging metrics for the inference worker, respectively the time spent on the last inference step and the time spent waiting for new observations from the rollout workers, both in seconds.</li> </ul>"},{"location":"05-monitoring/metrics-reference/#train","title":"<code>train</code>","text":"<p>This is perhaps the most useful section of metrics, many parameters can be used to debug RL training issues. Metrics are listed and explained below in the alphabetical order in which they appear in Tensorboard.</p> <p></p> <ul> <li><code>train/actual_lr</code> - the actual learning rate used by the learner, which can be different from the configuration parameter if the adaptive learning rate is enabled.</li> <li><code>train/adam_max_second_moment</code> - the maximum value of the second moment of the Adam optimizer. Sometimes spikes in this metric can  be used to detect training instability.</li> <li><code>train/adv_max</code>, <code>train/adv_min</code>, <code>train/adv_std</code> - the maximum, minimum, standard deviation of the advantage values. \"Mean\" value is not reported because it is always zero (we use advantage normalization by default).</li> <li><code>train/entropy</code> - the entropy of the actions probability distribution.</li> <li><code>train/exploration_loss</code> - exploration loss (if any). See <code>--exploration_loss</code> argument for more details.</li> <li><code>train/fraction_clipped</code> - fraction of minibatch samples that were clipped by the PPO loss. This value growing too large is often a sign of training instability (i.e. learning rate is too high).</li> <li><code>train/grad_norm</code> - the L2 norm of the gradient of the loss function after gradient clipping.</li> <li><code>train/kl_divergence</code> - the average KL-divergence between the policy that collected the experience and the latest copy of the policy on the learner. This value growing or spiking is often concerning and can be a sign of training instability.</li> <li><code>train/kl_divergence_max</code> - max KL value in the whole minibatch.</li> <li><code>train/kl_loss</code> - value of the KL loss (if any). See <code>--kl_loss_coeff</code> argument for more details.</li> </ul> <p></p> <ul> <li><code>train/loss</code> - the total loss function value.</li> <li><code>train/lr</code> - the learning rate used by the learner (can be changed by PBT algorithm even if there is no lr scheduler).</li> <li><code>train/max_abs_logprob</code> - the maximum absolute value of the log probability of any action in the minibatch under the latest policy. If this reaches hundreds or thousands (extremely improbable) it might be a sign that the distributions fluctuate too much, although it can also happen with very complex action distributions, i.e. Tuple action distributions.</li> <li><code>train/measurements_running_mean</code>, <code>train/measurements_running_std</code> - in this particular example the environment provides the additional observation space called \"measurements\" and these values report the statistics of this observation space.</li> <li><code>train/num_sgd_steps</code> - number of SGD steps performed on the current trajectories dataset when the summaries are recorded. This can range from 1 to <code>--num_epochs</code> * <code>--num_batches_per_epoch</code>.</li> <li><code>train/obs_running_mean</code>, <code>train/obs_running_std</code> - the running mean and standard deviation of the observations, reported  when <code>--normalize_input</code> is enabled.</li> <li><code>train/policy_loss</code> - policy gradient loss component of the total loss.</li> <li><code>train/ratio_max</code>, <code>train/ratio_mean</code>, <code>train/ratio_min</code> - action probability ratio between the latest policy and the policy that collected the experience. Min/max/mean are across the minibatch.</li> </ul> <p></p> <ul> <li><code>train/returns_running_mean</code>, <code>train/returns_running_std</code> - the running mean and standard deviation of bootstrapped discounted returns, reported when <code>--normalize_returns</code> is enabled.</li> <li><code>train/same_policy_fraction</code> - fraction of samples in the minibatch that come from the same policy. This can be less than 1.0 in multi-policy (i.e. PBT) workflows when we change the policy controlling the agent mid-episode.</li> <li><code>train/valids_fraction</code> - fraction of samples in the minibatch that are valid. Samples can be invalid if they come from a different policy or if they are too old exceeding <code>--max_policy_lag</code>. In most cases both <code>train/same_policy_fraction</code> and <code>train/valids_fraction</code> should be close to 1.0.</li> <li><code>train/value</code> - discounted return as predicted by the value function.</li> <li><code>train/value_delta</code>, <code>train/value_delta_max</code> - how much the value estimate changed between the current critic and the critic at the moment when the experience was collected. Similar to <code>train/ratio...</code> metrics, but for the value function.</li> <li><code>train/value_loss</code> - value function loss component of the total loss.</li> <li><code>train/version_diff_avg</code>, <code>train/version_diff_max</code>, <code>train/version_diff_min</code> - policy lag measured in policy versions (SGD steps) between the policy that collected the experience and the latest policy on the learner.</li> </ul>"},{"location":"05-monitoring/tensorboard/","title":"Tensorboard","text":"<p>Sample Factory uses Tensorboard summaries. Run Tensorboard to monitor any running or finished experiments:</p> <pre><code>tensorboard --logdir=&lt;your_train_dir&gt; --port=6006\n</code></pre>"},{"location":"05-monitoring/tensorboard/#monitoring-multiple-experiments","title":"Monitoring multiple experiments","text":"<p>Additionally, we provide a helper script that has nice command line interface to monitor select experiment folders using wildcards:</p> <pre><code>python -m sample_factory.utils.tb --dir=./train_dir '*name_mask*' '*another*mask*'\n</code></pre> <p>Here <code>--dir</code> parameter is the root directory with experiments, and the script will recursively search for experiment folders that match the masks.</p>"},{"location":"05-monitoring/tensorboard/#monitoring-experiments-started-by-the-launcher","title":"Monitoring experiments started by the Launcher","text":"<p>Launcher API is a convenient way to start multiple experiments in parallel. Such groups of experiments can be monitored with a single Tensorboard command, just specify <code>--logdir</code> pointing to the root directory with experiments.</p>"},{"location":"05-monitoring/wandb/","title":"Weights and Biases","text":"<p>Sample Factory also supports experiment monitoring with Weights and Biases. In order to setup WandB locally run <code>wandb login</code> in the terminal (https://docs.wandb.ai/quickstart#1.-set-up-wandb)</p> <p>Example command line to run an experiment with WandB monitoring:</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_basic --experiment=DoomBasic --train_dir=./train_dir --num_workers=20 --num_envs_per_worker=16 --train_for_env_steps=1000000 \\\\\n --with_wandb=True --wandb_user=&lt;your_wandb_user&gt; --wandb_tags test doom appo\n</code></pre> <p>A total list of WandB settings:  <pre><code>--with_wandb: Enables Weights and Biases integration (default: False)\n--wandb_user: WandB username (entity). Must be specified from command line! Also see https://docs.wandb.ai/quickstart#1.-set-up-wandb (default: None)\n--wandb_project: WandB \"Project\" (default: sample_factory)\n--wandb_group: WandB \"Group\" (to group your experiments). By default this is the name of the env. (default: None)\n--wandb_job_type: WandB job type (default: SF)\n--wandb_tags: [WANDB_TAGS [WANDB_TAGS ...]] Tags can help with finding experiments in WandB web console (default: [])\n</code></pre></p> <p>Once the experiment is started the link to the monitored session is going to be available in the logs (or you can find it by searching in Wandb Web console).</p>"},{"location":"05-troubleshooting/troubleshooting/","title":"Troubleshooting / FAQ","text":"<p>Frequently asked questions and answers.</p>"},{"location":"05-troubleshooting/troubleshooting/#fps-and-throughput-drop-to-0-shortly-after-startup","title":"FPS and throughput drop to 0 shortly after startup","text":"<p>This is caused by one of the key system components crashing during startup. Examples include: all rollout workers crash due to a bug in the enviroment creation code or <code>reset()</code>, learner crashes  due to CUDA or OOM issues, or any other similar reason.</p> <p>To diagnose the problem you should check the full log file to see exactly what happened (i.e. to find the exception trace). Stdout/stderr output can be viewed in the terminal, or in the <code>sf_log.txt</code> file in the experiment folder.</p> <p>Try to look for the original issue that caused the system to halt; periodic log messages may cause it not to appear on the screen at the time the problem is noticed.</p>"},{"location":"06-architecture/message-passing/","title":"Event Loops, Signals, and Slots","text":"<p>Sample Factory uses a custom mechanism for communication between components inspired by Qt's signals and slots. Unlike in Qt, signals and slots can be used not only across threads, but also across processes. The implementation of this mechanism is available as a separate repository here.</p> <p>The main idea can be summarised as follows:</p> <ul> <li>Application is a collection of <code>EventLoop</code>s. Each <code>EventLoop</code> is an infinite loop that occupies a thread or a process.</li> <li>Logic of the system is implemented in <code>EventLoopObject</code> components that live on <code>EventLoop</code>s. Each <code>EventLoop</code> can support multiple <code>EventLoopObject</code>s.</li> <li>Components (i.e. <code>EventLoopObject</code>s) can emit signals. A signal \"message\" contains a name of the signal and the payload (arbitrary data).</li> <li>Components can also connect to signals emitted by other components by specifying a <code>slot</code> function to be called when the signal is received by the EventLoop.</li> </ul> <p>The majority of communication between components is done via signals and slots. Some examples:</p> <ul> <li>Rollout workers emit <code>\"p{policy_id}_trajectories\"</code> signal when a new trajectory is available, and Batcher's <code>on_new_trajectories()</code> slot is connected to this signal.</li> <li>Inference workers emit <code>\"advance{rollout_worker_idx}\"</code> signal when actions are ready for the next rollout step, and RolloutWorker's <code>advance_rollouts()</code> slot is connected to this signal.</li> </ul>"},{"location":"06-architecture/message-passing/#implementation-details","title":"Implementation details","text":"<ul> <li>There's no argument validation for signals and slots. If you connect a slot to a signal with a different signature, it will fail at runtime. This can also be used to your advantage by allowing to propagate arbitrary data as payload with appropriate runtime checks.</li> <li>Signals can be connected to slots only before the processes are spawned, i.e. only during system initialization. This is mostly done by the <code>Runner</code> in <code>connect_components()</code>.</li> <li>It is currently impossible to connect a slot to a signal if emitter and receiver objects belong to event loops already running in different processes (although it should be possible to implement this feature). Connect signals to slots during system initialization.</li> <li>Signal-slot mechanism in the current implementation can't implement a message passing protocol where only a single copy of the signal is received by the subscribers. Signals are always delivered to all connected slots. Use a FIFO multiprocessing queue if you want only one receiver to receive the signal. For example, RolloutWorkers explicitly push requests for new actions into queues corresponding to a policy that controls the agent, and this queue can be processed by any of the multiple InferenceWorkers: <code>inference_queues[policy_id].put(policy_request)</code></li> </ul> <p>Please see https://github.com/alex-petrenko/signal-slot for more information.</p>"},{"location":"06-architecture/message-passing/#multiprocessing-queues","title":"Multiprocessing queues","text":"<p>At the core of the signal-slot mechanism are the queues that are used to pass messages between processes. Python provides a default implementation <code>multiprocessing.Queue</code>, which turns out to be rather slow.</p> <p>Sample Factory uses a custom queue implementation written in C++ using POSIX API that is significantly faster: https://github.com/alex-petrenko/faster-fifo</p>"},{"location":"06-architecture/overview/","title":"Architecture Overview","text":"<p>While a basic implementation of an RL algorithm can fit in a single file, a high-throughput RL system requires a rather sophisticated architecture. This document describes the high-level design of Sample Factory.</p> <p>The following diagram shows the main components of the system and the data flow between them. Please see sections below for more details.</p> <p></p>"},{"location":"06-architecture/overview/#high-level-design","title":"High-level design","text":"<p>At the core of Sample Factory structure is the idea that RL training can be split into multiple largely independent components, each one of them focusing on a specific task. This enables a modular design where these components can be accelerated/parallelized independently, allowing us to achieve the maximum performance on any RL task.</p> <p>Components interact asynchronously by sending and receving messages (aka signals, see a dedicated section on message passing). Typically separate components live on different event loops in different processes, although the system is agnostic of whether this is true and it is thus possible to run multiple (or even all components) on a single event loop in a single process. </p> <p>Instead of explicitly sending the data between components (i.e. by serializing observations and sending them across processes), we choose to send the data through shared memory buffers. Each time a component needs to send data to another component, it writes the data to a shared memory buffer and sends a signal containing the buffer ID (essentially a pointer to data). This massively reduces the overhead of message passing.</p>"},{"location":"06-architecture/overview/#components","title":"Components","text":"<p>Each component is dedicated to a specific task and can be seen as a data processing engine (i.e. each component gets some input by receiving signals, executes a computation, and broadcasts the results by emitting its own signals).</p> <p>These are the main components of Sample Factory:</p> <ul> <li>Rollout Workers are responsible for environment simulation. Rollout workers receive actions from the policy, do environment <code>step()</code> and produce observations after each step and full trajectories after <code>--rollout</code> steps.</li> <li>Inference Workers receive observations and hidden states and produce actions. The policy on each inference worker is updated after each SGD step on the learner.</li> <li>Batcher receives trajectories from rollout workers, puts them together and produces datasets of data for the learner.</li> <li>Learner gets batches of data from the batcher, splits them into minibatches and does <code>--num_epochs</code> of stochastic gradient descent. After each SGD step the updated weights are written to shared memory buffers and the corresponding signal is broadcasted.</li> <li> <p>Runner is a component that bootstraps the whole system, receives all sorts of statistics from other components and takes care of logging and summary writing.</p> </li> <li> <p>Sampler, although technically its own component that can send and receive signals, in the typical configuration is nothing more than a thin wrapper around Rollout/Inference workers and serves as an interface to the rest of the system. (Although this interface allows us to create alternative samplers i.e. single-process synchronous JAX-optimized sampler is an idea)</p> </li> </ul>"},{"location":"06-architecture/overview/#rollout-workers","title":"Rollout Workers","text":"<p>The number of rollout workers is controlled by <code>--num_workers</code>. Each rollout worker can simulate one or multiple environments serially in the same process. The number of environments per worker is controlled by <code>--num_envs_per_worker</code>.</p> <p>Each rollout worker contains &gt;= 1 of <code>VectorEnvRunner</code> objects, the number of which is controlled by <code>--worker_num_splits</code>. The default value of this parameter is 2, which enables double-buffered sampling. The number of envs on each <code>VectorEnvRunner</code> is thus <code>num_envs_per_worker // worker_num_splits</code> and therefore <code>--num_envs_per_worker</code> must be divisible by <code>--worker_num_splits</code>.</p>"},{"location":"06-architecture/overview/#inference-workers","title":"Inference Workers","text":"<p>Each policy (see multi-policy training) has &gt;= 1 corresponding inference workers which generate actions for the agents controlled by this policy. The number of inference workers is controlled by <code>--policy_workers_per_policy</code>.</p>"},{"location":"06-architecture/overview/#batcher","title":"Batcher","text":"<p>There's typically a single batcher per policy in the system. The batcher receives trajectories from rollout workers and puts them together into a dataset available for training. In batched sampling mode this is pretty much a no-op, the batcher just passes the data through. In non-batched sampling mode this is a non-trivial process, since rollouts from different workers finish asynchronously and need to be put in the contiguous tensor for minibatch SGD.</p> <p>Although batcher is it's own component, in the default configuration we run it in the same process as the learner (but in a separate thread) in order to minimize the number of CUDA contexts and thus VRAM usage.</p>"},{"location":"06-architecture/overview/#learner","title":"Learner","text":"<p>There's typically a single learner per policy in the system. Trajectory datasets flow in and updated parameters flow out.</p>"},{"location":"06-architecture/overview/#terminology","title":"Terminology","text":"<p>Some terminology used in the codebase and in the further documentation:</p> <ul> <li>rollout or trajectory is a sequence of observations, actions, rewards, etc. produced by a single agent.</li> <li>dataset (or training batch or sometimes just batch) is a collection of trajectories produced by &gt;=1 agents.</li> <li>Datasets are split into minibatches and &gt;=1 epochs of SGD are performed. Minibatch size is determined by <code>--batch_size</code> and number of epochs is determined by <code>--num_epochs</code>. Dataset size is <code>batch_size * num_batches_per_epoch</code>, and in total <code>batch_size * num_batches_per_epoch * num_epochs</code> SGD steps are performed on each dataset (sorry for the obvious confusion between \"batch\" and \"minibatch\" terms, the parameter names are kept largely for legacy reasons).</li> <li>signals are messages sent between components. Signals are connected to slots, which are functions that are called when a signal is received. This mechanism is inspired by Qt's signals and slots (see the dedicated section on message passing).</li> <li>shared memory buffers are PyTorch tensors shared between processes, created with <code>share_memory_()</code> method.</li> </ul>"},{"location":"07-advanced-topics/batched-non-batched/","title":"Batched/Non-Batched Sampling","text":"<p>Sample Factory has two different implementations of the RolloutWorker: batched and non-batched. You can switch between them using <code>--batched_sampling=[True|False]</code> argument.</p>"},{"location":"07-advanced-topics/batched-non-batched/#non-batched-sampling","title":"Non-Batched Sampling","text":"<p>Non-batched sampling is the default mode. It makes very few assumptions about the environment and is the most flexible. We call it non-batched because it treats each agent in each environment independently and processes trajectories one by one.</p> <p>One advantage of this approach is that we can control each agent in each environment with any policy which can be very useful for multi-agent, self-play, and PBT setups.</p> <p>A downside of this mode is that we have to batch individual observations from rollout workers before we can do inference on the GPU (because GPUs are most efficient with big batches). This makes non-batched mode very inefficient for vectorized environments like IsaacGym.</p>"},{"location":"07-advanced-topics/batched-non-batched/#batched-sampling","title":"Batched Sampling","text":"<p>Batched mode is perfect for massively vectorized environments like IsaacGym or EnvPool. It assumes that the observations are available in one large tensor that we can directly give to the inference worker for processing.</p> <p>For GPU-accelerated environments we can sample thousands of observations in a single tensor that is already on GPU and thus achieve the maximum possible throughput. It is common that batched mode is used with a single rollout worker <code>--num_workers=1</code> and a single inference worker <code>--num_inference_workers=1</code> and the parallelization of environment simulation is handled by the environment itself.</p> <p>Although PBT can be used with batched sampling, we do not support controlling individual agents with different policies in this mode.</p> <p>For regular CPU-based envs (Atari, VizDoom, Mujoco) the difference between batched and non-batched sampling is negligible, either mode should work fine.</p>"},{"location":"07-advanced-topics/batched-non-batched/#observations","title":"Observations","text":"<p>In Sample Factory for simplicity all environments have dictionary observations (or converted to dictionary with a single key <code>obs</code>). In non-batched mode even with multi-agent envs each agent thus provides a separate observation dictionary.</p> <p>In batched mode we want to work with big tensors, so instead of a list of dictionaries we have a single dictionary with a tensor of observations for each key.</p>"},{"location":"07-advanced-topics/double-buffered/","title":"Double-Buffered Sampling","text":"<p>Sample Factory supports accelerated sampling regime called double-buffered sampling which can be enabled by setting <code>--worker_num_splits=2</code> (default value) and <code>--num_envs_per_worker</code> to a multiple of 2.</p> <p>Note that this feature is independent of sync/async or batched/non-batched mode and can be used in any configuration.</p>"},{"location":"07-advanced-topics/double-buffered/#explanation","title":"Explanation","text":"<p>Experience collection in RL is normally a sequential process. We can't collect the next observation until we generate an action based on the current observation.</p> <p>This means that for CPU-based environments we can't use our CPU cores when we're waiting for  the inference to finish. This is a waste of resources and it slows down training.</p> <p>Double-buffered sampling solves this problem by simulating 2*N environments serially in the same process. While we're waiting for the inference to finish on the first N environments, we can already collect observations from the next N environments.</p> <p>The diagram below shows how this works:</p> <p></p> <p>Additionally, take a look at this animation that demonstrates double-buffered sampling: https://www.youtube.com/watch?v=0AyaeLqXQc4</p>"},{"location":"07-advanced-topics/inactive-agents/","title":"Inactive Agents","text":"<p>Sometimes it makes sense to disable some of the agents in a multi-agent environment. For example, in a multi-player game some agents might die in the middle of the episode and should not contribute any rollouts until the episode reset.</p> <p>In order to disable (deactivate) the agent, add <code>info[\"is_active\"] = False</code> in the <code>env.step()</code> call, i.e. the agent's info dict should contain <code>is_active</code> key with <code>False</code> value. Absent <code>is_active</code> key or <code>is_active=True</code> is treated as active agent.</p> <p>When the agent is deactivated in the middle of the rollout, the inactive part of the rollout is treated as <code>invalid</code> data by the learner (similar to any other invalid data, i.e. experience that exceeds <code>--max_policy_lag</code>).</p> <p>We carefully mask this invalid data on the learner for loss &amp; advantages calculations. Therefore any inactive data makes the effective batch size smaller, so we decrease the learning rate accordingly, otherwise batches with &gt;90% invalid data would produce very noisy parameter updates.</p> <p>It is generally advised that the portion of inactive data (<code>train/valids_fraction</code> on Tensorboard/WandB) does not exceed 50%, otherwise it may seriously affect training dynamics and requires careful tuning.</p> <p>There are also alternative ways to treat inactive agents, for example just feeding them some special observation (e.g. all zeros) and zero rewards until the episode reset.</p> <p>Inactive agents are currently only supported in non-batched sampling mode (<code>--batched_sampling=False</code>).</p>"},{"location":"07-advanced-topics/inactive-agents/#examples","title":"Examples","text":"<ul> <li><code>sf_examples/train_custom_multi_env.py</code> - shows how to use inactive agents in a custom multi-agent environment.</li> </ul> <p>Inactive agents are a new feature, suggestions &amp; contributions are welcome!</p>"},{"location":"07-advanced-topics/multi-policy-training/","title":"Multi-Policy Training","text":"<p>Sample Factory supports training multiple policies at the same time with <code>--num_policies=N</code>, where <code>N</code> is the number of policies to train.</p>"},{"location":"07-advanced-topics/multi-policy-training/#single-agent-environments","title":"Single-agent environments","text":"<p>Multi-policy training with single-agent environments is equivalent to just running multiple experiments with different seeds. We actually recommend running separate experiments in this case because experiment monitoring is easier this way.</p>"},{"location":"07-advanced-topics/multi-policy-training/#multi-agent-environments","title":"Multi-agent environments","text":"<p>In multi-agent and self-play environments it can be beneficial to train multiple policies at once to avoid overfitting to a single opponent (i.e. self).</p> <p>If this is the desired training mode, it is important that we control multiple agents in the same environment with different policies. This is controlled by the argument <code>--pbt_mix_policies_in_one_env</code>, which is set to <code>True</code> by default. Although this argument has <code>--pbt</code> prefix, it actually applies regardless of whether we're training with PBT or not.</p> <p>If <code>--pbt_mix_policies_in_one_env=True</code>, then we will periodically randomly resample policies controlling each agent in the environment. This is implemented in <code>sample_factory.algo.utils.agent_policy_mapping</code>. Feel free to fork the repo and modify this class to create your own custom mapping.</p> <p>Exposing <code>agent_policy_mapping</code> through API to allow custom mapping is an obvious improvement, and contributions here are welcome!</p>"},{"location":"07-advanced-topics/multi-policy-training/#implementation-details","title":"Implementation details","text":""},{"location":"07-advanced-topics/multi-policy-training/#gpu-mapping","title":"GPU mapping","text":"<p>On a multi-GPU machine we assign each policy to a separate GPU. Or, if we have fewer GPUs than policies, we will  fill the GPUs with policies until we run out of GPUs, and then start reusing GPUs.</p> <p>For example, on a 2-GPU machine 4-policy training will look like this:</p> <pre><code>GPU 0: policy 0, policy 2\nGPU 1: policy 1, policy 3\n</code></pre>"},{"location":"07-advanced-topics/multi-policy-training/#multi-policy-training-in-different-modes","title":"Multi-policy training in different modes","text":"<p>All features of multi-policy training (including mixing different policies in one env) are only supported with asynchronous (<code>--async_rl=True</code>) non-batched (<code>--batched_sampling=False</code>) training.</p> <p>In synchronous mode we can still use multi-policy training, but the mapping between agents and policies is fixed and deterministic because we need to guarantee the same amount of experience for all policies.</p> <p>In batched mode we can also use multi-policy training, but mixing policies in one environment is not supported. This would defeat the purpose of batched mode where we want to directly transfer a large vector of observations on the GPU and do inference. Arbitrary mapping between agents and policies would make this significantly slower and more complicated.</p> <p>That said, it might still make a lot of sense to use multi-policy training in batched mode/sync mode in the context of Population-Based Training, i.e. to optimize hyperparameters of agents in the population.</p> <p>See Population-Based Training for more details.</p>"},{"location":"07-advanced-topics/normalizations/","title":"Observation and Return Normalization","text":"<p>Neural networks are known to perform better when the input data and prediction targets are normalized.</p> <p>Sample Factory provides normalization for observations and returns. Normalization works by collecting running mean and standard deviation statistics of the data for the entire training process, and then using these statistics to normalize the data to approximately zero mean and unit variance.</p> <p>This way of normalization has proven more effective than per-batch normalization since it changes the statistics slowly and smoothly as observations and returns change, and thus allows the networks to adapt to the new statistics.</p>"},{"location":"07-advanced-topics/normalizations/#observation-normalization","title":"Observation normalization","text":"<p>Enable observation normalization by setting <code>--normalize_input</code> to <code>True</code>. If your environment provides dictionary observations, you can specify which keys to normalize by setting <code>--normalize_input_keys key1 key2 key3</code> (defaults to all keys).</p> <p>Observation normalization in some cases can massively improve sample efficiency. I.e. below is VizDoom \"doom_basic\" environment with (blue) and without (orange) observation normalization:</p> <p></p>"},{"location":"07-advanced-topics/normalizations/#return-normalization","title":"Return normalization","text":"<p>Enable return normalization by setting <code>--normalize_returns</code> to <code>True</code>. In addition to stabilizing training and reducing the critic loss, return normalization  eliminates the need for careful tuning of reward scaling.</p> <p>In the example below we train an agent on a relatively complex continuous control task, and the version with return normalization not only trains faster, but also consistently reaches much higher reward.</p> <p></p> <p>There's no guarantee that normalization of observations and returns will always help, experiment with your environment to see if it helps.</p>"},{"location":"07-advanced-topics/normalizations/#advantage-normalization","title":"Advantage normalization","text":"<p>In addition to observation and return normalization, Sample Factory also normalizes advantages. Unlike observation and return normalization, advantage normalization is not based on running statistics, but instead uses per-batch statistics. We found that this configuration performs well in many different domains.</p>"},{"location":"07-advanced-topics/observer/","title":"Observer Interface","text":"<p>Sample Factory version 2 introduces a new feature: you can wrap the RL algorithm with a custom <code>Observer</code> object which allows you to interact with the RL training process in an arbitrary way.</p>"},{"location":"07-advanced-topics/observer/#algoobserver","title":"<code>AlgoObserver</code>","text":"<p>The <code>AlgoObserver</code> interface is defined as follows:</p> <pre><code>class AlgoObserver:\n    def on_init(self, runner: Runner) -&gt; None:\n        \"\"\"Called after ctor, but before signal-slots are connected or any processes are started.\"\"\"\n        pass\n\n    def on_connect_components(self, runner: Runner) -&gt; None:\n        \"\"\"Connect additional signal-slot pairs in the observers if needed.\"\"\"\n        pass\n\n    def on_start(self, runner: Runner) -&gt; None:\n        \"\"\"Called right after sampling/learning processes are started.\"\"\"\n        pass\n\n    def on_training_step(self, runner: Runner, training_iteration_since_resume: int) -&gt; None:\n        \"\"\"Called after each training step.\"\"\"\n        pass\n\n    def extra_summaries(self, runner: Runner, policy_id: PolicyID, env_steps: int, writer: SummaryWriter) -&gt; None:\n        pass\n\n    def on_stop(self, runner: Runner) -&gt; None:\n        pass\n</code></pre> <p>Define your own class derived from <code>AlgoObserver</code> (i.e. <code>MyObserver</code>) and register it before starting the training process:</p> <pre><code>runner.register_observer(MyObserver())\n</code></pre> <p>Our DMLab integration provides an example of how to use <code>AlgoObserver</code> to implement custom summaries that  aggregate information from multiple custom metrics (see <code>sf_examples/dmlab/train_dmlab.py</code>).</p> <p><code>AlgoObserver</code> is a new feature and further suggestions/extensions are welcome!</p>"},{"location":"07-advanced-topics/passing-info/","title":"Passing Info from RL Algo to Env","text":"<p>Custom summary metrics provide a way to pass information from the RL environment to the training system (i.e. success rate, etc.)</p> <p>In some RL workflows it might be desirable to also pass information in the opposide direction: from the RL algorithm to the environment. This can enable, for example, curriculum learning based on the number of training steps consumed by the agent (or any other metric of the training progress).</p> <p>We provide a way to do this by passing a <code>training_info</code> dictionary to the environment. In order to do this, your environment needs to implement <code>TrainingInfoInterface</code>.</p> <pre><code>class TrainingInfoInterface:\n    def __init__(self):\n        self.training_info: Dict[str, Any] = dict()\n\n    def set_training_info(self, training_info):\n        \"\"\"\n        Send the training information to the environment, i.e. number of training steps so far.\n        Some environments rely on that i.e. to implement curricula.\n        :param training_info: dictionary containing information about the current training session. Guaranteed to\n        contain 'approx_total_training_steps' (approx because it lags a bit behind due to multiprocess synchronization)\n        \"\"\"\n        self.training_info = training_info\n</code></pre> <p>Currently we only pass <code>approx_total_training_steps</code> to the environment which should be enough for simple curricula. Feel free to fork the repo and add more information to this dictionary by modifying <code>_propagate_training_info()</code> in <code>runner.py</code>. This is a new feature and further suggestions/extensions are welcome.</p> <p>Note that if your environment uses a chain of wrappers (e.g. <code>env = Wrapper3(Wrapper2(Wrapper1(env)))</code>), then it is sufficient that any Wrapper in the chain implements <code>TrainingInfoInterface</code>. Sample Factory will unwrap the outer wrappers until it finds the first one that implements <code>TrainingInfoInterface</code>.</p>"},{"location":"07-advanced-topics/passing-info/#additional-notes-on-curriculum-learning","title":"Additional notes on curriculum learning","text":"<p>Curriculum based on the training progress is not the only way to implement curriculum learning. In most cases, you can actually do it without knowing anything about the outer training loop.</p> <p>An alternative approach is to implement curriculum based on the agent's performance in the current environment instance, i.e. by averaging episodic statistics over the last N episodes. This way the resulting curriculum is more smooth and stochastic, which can actually create more robust policies, since different environment instances can be at different levels of difficulty and thus produce more diverse data. We used this approach to train our agents against bots in the original Sample Factory paper.</p>"},{"location":"07-advanced-topics/pbt/","title":"Population-Based Training","text":"<p>Sample Factory contains an implementation of the Population-Based Training algorithm. See PBT paper and original Sample Factory paper for more details.</p> <p>PBT is a hyperparameter optimization algorithm that can be used to train RL agents. Instead of manually tuning all hyperparameters, you can let an optimization method do it for you. This can include  not only learning parameters (e.g. learning rate, entropy coefficient), but also environment parameters (e.g. reward function coefficients).</p> <p>It is common in RL to have a sophisticated (shaped) reward function which guides the exploration process. As a result such reward function can distract the agent from the actual final goal.</p> <p>PBT allows you to optimize with respect to some sparse final objective (which we call \"true_objective\") while still using a shaped reward function. Theoretically the algorithm should find hyperparameters (including shaping coefficients) that lead to the best final objective. This can be, for example, directly optimizing for just winning a match in a multiplayer game, which would be very difficult to do with just regular RL because of the sparsity of such objective. This type of PBT algorithm is implemented in the FTW agent by DeepMind.</p>"},{"location":"07-advanced-topics/pbt/#algorithm","title":"Algorithm","text":"<p>PBT works similar to a genetic algorithm. A population of agents is trained simultaneously with roughly the following approach:</p> <ul> <li>Each agent is assigned a set of hyperparameters (e.g. learning rate, entropy coefficient, reward function coefficients, etc.)</li> <li>Each agent is trained for a fixed number of steps (e.g. 5M steps)</li> <li>At the end of this meta-training epoch, the performance of all agents is ranked:<ul> <li>Agents with top K % of performance are unchanged, we just keep training them</li> <li>Agents with bottom K % of performance are replaced by a copy of a random top-K % agent with mutated hyperparameters.</li> <li>Agents in the middle keep their weights but also get mutated hyperparameters.</li> </ul> </li> <li>Proceed to the next meta-training epoch.</li> </ul> <p>Current version of PBT is implemented for a single machine. The perfect setup is a multi-GPU server that can train multiple agents at the same time. For example, we can train a population of 8 agents on a 4-GPU machine, training 2 agents on each GPU.</p> <p>PBT is perfect for multiplayer game scenarios where training a population of agents against one another yields much more robust results compared to self-play with a single policy.</p>"},{"location":"07-advanced-topics/pbt/#providing-true-objective-to-pbt","title":"Providing \"True Objective\" to PBT","text":"<p>In order to optimize for a true objective, you need to return it from the environment. Just add it to the <code>info</code> dictionary returned by the environment at the last step of the episode, e.g.:</p> <pre><code>def step(self, action):\n    info = {}\n    ...\n    info['true_objective'] = self.compute_true_objective()\n    return obs, reward, terminated, truncated, info\n</code></pre> <p>In the absence of <code>true_objective</code> in the <code>info</code> dictionary, PBT will use the regular reward as the objective.</p>"},{"location":"07-advanced-topics/pbt/#learning-parameters-optimized-by-pbt","title":"Learning parameters optimized by PBT","text":"<p>See <code>population_based_training.py</code>:</p> <pre><code>HYPERPARAMS_TO_TUNE = {\n    \"learning_rate\",\n    \"exploration_loss_coeff\",\n    \"value_loss_coeff\",\n    \"max_grad_norm\",\n    \"ppo_clip_ratio\",\n    \"ppo_clip_value\",\n    # gamma can be added with a CLI parameter (--pbt_optimize_gamma=True)\n}\n</code></pre> <p>During training current learning parameters are saved in <code>f\"policy_{policy_id:02d}_cfg.json\"</code> files in the experiment directory.</p>"},{"location":"07-advanced-topics/pbt/#optimizing-environment-parameters","title":"Optimizing environment parameters","text":"<p>Besides learning parameters we can also optimize parameters of the environment with respect to some \"true objective\".</p> <p>In order to do that, your environment should implement <code>RewardShapingInterface</code> interface in addition to <code>gym.Env</code> interface.</p> <pre><code>class RewardShapingInterface:\n    def get_default_reward_shaping(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Should return a dictionary of string:float key-value pairs defining the current reward shaping scheme.\"\"\"\n        raise NotImplementedError\n\n    def set_reward_shaping(self, reward_shaping: Dict[str, Any], agent_idx: int | slice) -&gt; None:\n        \"\"\"\n        Sets the new reward shaping scheme.\n        :param reward_shaping dictionary of string-float key-value pairs\n        :param agent_idx: integer agent index (for multi-agent envs). Can be a slice if we're training in batched mode\n        (set a single reward shaping scheme for a range of agents)\n        \"\"\"\n        raise NotImplementedError\n</code></pre> <p>Any parameters in the dictionary returned by <code>get_default_reward_shaping</code> will be optimized by PBT. Note that although the dictionary is called \"reward shaping\", it can be used to optimize any environment parameters.</p> <p>It is important that none of these parameters should directly affect the objective calculation, otherwise all PBT will do is increase the coefficients all the way to infinity.</p> <p>An example of how this can be used. Suppose your shaped reward function contains a term for picking up a weapon in a game like Quake or VizDoom. If the true objective is <code>1.0</code> for winning the game and <code>0.0</code> otherwise then PBT can optimize these weapon preference coefficients to maximize success. But if true objective is not specified (so just the env reward itself is used as objective), then you can just increase the coefficients to increase the reward unboundedly.</p>"},{"location":"07-advanced-topics/pbt/#configuring-pbt","title":"Configuring PBT","text":"<p>Please see Configuration parameter reference. Parameters with <code>--pbt_</code> prefix are related to PBT. Use <code>--with_pbt=True</code> to enable PBT. It is important also to set <code>--num_policies</code> to the number of agents in the population.</p>"},{"location":"07-advanced-topics/pbt/#command-line-examples","title":"Command-line examples","text":"<p>Training on DMLab-30 with a 4-agent population on a 4-GPU machine:</p> <pre><code>python -m sf_examples.dmlab.train_dmlab --env=dmlab_30 --train_for_env_steps=10000000000 --algo=APPO --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache\n</code></pre> <p>PBT for VizDoom (8 agents, 4-GPU machine):</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots\n</code></pre>"},{"location":"07-advanced-topics/policy-lag/","title":"Policy Lag","text":"<p>Policy lag is the discrepancy between the policy that is used to collect samples and the policy that we train on this data. Policy gradient algorithms (like PPO) are considered on-policy methods and typically suffer sample efficiency losses when the policy lag is large (experience is stale, \"off-policy\").</p> <p>In practice PPO is pretty robust to slightly off-policy data, but it is important to keep the policy lag under control.</p> <p>Although this is not rigorous, we can measure the policy lag in the number of SGD steps between the policy that collected the data (behavior policy) and the trained policy (target policy).</p>"},{"location":"07-advanced-topics/policy-lag/#sources-of-policy-lag","title":"Sources of policy lag","text":"<p>There are following main sources of policy lag:</p> <ul> <li> <p>Multiple updates on the same data. This is the most common source of policy lag inherent to almost all policy gradient implementations. If <code>--num_batches_per_epoch &gt; 1</code> and/or <code>--num_epochs &gt; 1</code> then we need to do multiple SGD steps before we finish training on the sampled data, causing the lag in the later epochs.</p> </li> <li> <p>Collecting more experience per sampling iteration (one rollout for all agents) than we use for one iteration of training   (<code>num_workers * num_envs_per_worker * rollout &gt;&gt; batch_size * num_batches_per_epoch</code>). In this case we will inevitably have some trajectories (or parts of trajectories) collected by the policy that is already outdated by the time we use them for training.</p> </li> <li> <p>Async sampling. With asynchronous sampling we collect new data while we are training on the old data, which will inevitably cause some amount of lag. This is the smallest source of lag since we update the policy on the inference worker as soon as new weights are available.</p> </li> </ul>"},{"location":"07-advanced-topics/policy-lag/#estimating-and-measuring-policy-lag","title":"Estimating and measuring policy lag","text":"<p>Policy lag for a particular RL experiment configuration is roughly proportional to the following value:</p> <pre><code>Lag ~~ (num_epochs * num_workers * num_envs_per_worker * agents_per_env * rollout) / batch_size\n</code></pre> <p>Sample Factory reports empirical policy lag in two different ways.</p>"},{"location":"07-advanced-topics/policy-lag/#policy-lag-measurements-printed-to-the-console","title":"Policy lag measurements printed to the console","text":"<pre><code>[2022-11-30 19:48:19,509][07580] Updated weights for policy 0, policy_version 926 (0.0015)                                                                                                                  \n[2022-11-30 19:48:21,166][07494] Fps is (10 sec: 22528.2, 60 sec: 20377.6, 300 sec: 20377.6). Total num frames: 3829760. Throughput: 0: 5085.4. Samples: 203415. Policy #0 lag: (min: 0.0, avg: 1.9, max: 5.0)                                                                                                           \n[2022-11-30 19:48:21,166][07494] Avg episode reward: [(0, '0.824')]\n</code></pre> <p>Here message <code>Policy #0 lag: (min: 0.0, avg: 1.9, max: 5.0)</code> contains the minimum, average and maximum policy lag for transitions encountered in the last minibatch processed by the learner at the moment of printing. This can correspond to a minibatch in earlier or later epochs, so these values might fluctuate, but looking at 5-10 consecutive printouts should give a good idea of the policy lag.</p>"},{"location":"07-advanced-topics/policy-lag/#policy-lag-measurements-in-tensorboard-or-weights-biases","title":"Policy lag measurements in Tensorboard or Weights &amp; Biases","text":"<p><code>train/version_diff_avg</code>, <code>train/version_diff_max</code>, <code>train/version_diff_min</code> metrics represent policy lag values measured in policy versions (SGD steps). See Metrics Reference.</p>"},{"location":"07-advanced-topics/policy-lag/#minimizing-policy-lag","title":"Minimizing policy lag","text":"<p>Policy lag can usually be traded off for higher throughput or sample efficiency (i.e. by doing many epochs of SGD on the same data). But large values of policy lag can cause instability in training. Each task will have its own sweet spot when it comes to configuration and policy lag. Very roughly speaking, policy lag &lt; 20-30 SGD steps is usually fine, but significantly larger values might be a reason to reconsider the configuration.</p> <p>Empirically, LSTM/GRU policies and environments with very complex action spaces tend to be more sensitive to policy lag. For RNNs this is true because not only the action distributions, but also the hidden states change between the behavior and target policies. With complex action spaces (i.e. tuple, multi-discrete) small changes to the policy can cause large changes to probabilities of individual actions.</p> <p>Following configuration options can be used to minimize policy lag:</p> <ul> <li>Increase <code>batch_size</code>, decrease <code>num_epochs</code>, <code>num_batches_per_epoch</code>, <code>num_workers</code>, <code>num_envs_per_worker</code>, <code>rollout</code>, <code>num_batches_per_epoch</code> (see the formula above).</li> <li>Switch to synchronous sampling (<code>--async_rl=False</code>). Note that this will likely increase the training time.</li> </ul>"},{"location":"07-advanced-topics/policy-lag/#achieving-zero-policy-lag-a2c","title":"Achieving zero policy lag (A2C)","text":"<p>It is possible to achieve zero policy lag by using <code>--async_rl=False</code> and <code>--num_batches_per_epoch=1</code> and <code>--num_epochs=1</code>. This will turn PPO into the algorithm known as A2C (Advantage Actor-Critic) which always trains on the most recent data. This should typically yield stable training, although might not be the best option in terms of throughput or sample efficiency.</p>"},{"location":"07-advanced-topics/profiling/","title":"Profiling","text":"<p>It is virtually impossible to optimize any system without measuring its performance and identifying the bottlenecks. This guide will show you how to profile your RL workload in different regimes.</p>"},{"location":"07-advanced-topics/profiling/#profiling-with-the-built-in-timing-tool","title":"Profiling with the built-in \"Timing\" tool","text":"<p>Sample Factory provides a simple class called <code>Timing</code> (see <code>sample_factory/utils/timing.py</code>) that can be used for high-level profiling to get a rough idea of where the compute cycles are spent.</p> <p>Core hotspots are already instrumented, but if you'd like to see a more elaborate picture, you can use the <code>Timing</code> class in your own code like this:</p> <pre><code>import time\n\ntiming = Timing(name=\"MyProfile\")\n\n# add_time() will accumulate time spent in the block\n# this is the most commonly used method\nwith timing.add_time(\"hotspot\"):\n    # do something\n    ...\n\n    # measure time spent in a subsection of code\n    # when we build the timing report, we'll generate a tree corresponding to the nesting\n    with timing.add_time(\"subsection1\"):\n        # do something\n        ...\n\n    with timing.add_time(\"subsection2\"):\n        # do something\n        ...\n\n# instead of accumulating time, this will measure the last time the block was executed\nwith timing.timeit(\"hotspot2\"):\n    # do something\n    ...\n\n# this will measure the average time spent in the block\nwith timing.time_avg(\"hotspot3\"):\n    # do something\n    ...\n\n# this will print the timing report\nprint(timing)\n</code></pre>"},{"location":"07-advanced-topics/profiling/#example-profiling-an-asynchronous-workload","title":"Example: profiling an asynchronous workload","text":"<p>Let's take a look at a typical RL workload: training an agent in a VizDoom pixel-based environment. We use the following command line and run it on a 6-core laptop with hyperthreading:</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --env_frameskip=4 --train_for_env_steps=4000000 \\\\\n--use_rnn=True --num_workers=12 --num_envs_per_worker=16 --num_policies=1 --num_epochs=1 --rollout=32 --recurrence=32 \\\\\n--batch_size=2048 --experiment=profiling --benchmark=True --decorrelate_envs_on_one_worker=False --res_w=128 --res_h=72 \\\\\n--wide_aspect_ratio=True --policy_workers_per_policy=1 --worker_num_splits=2 --batched_sampling=False \\\\\n--serial_mode=False --async_rl=True --policy_workers_per_policy=1\n</code></pre> <p>If we wait for this experiment to finish (in this case, after training for 4M env steps), we'll get the following timing report:</p> <pre><code>[2022-11-25 01:36:52,563][15762] Batcher 0 profile tree view:\nbatching: 10.1365, releasing_batches: 0.0136\n[2022-11-25 01:36:52,564][15762] InferenceWorker_p0-w0 profile tree view:\nwait_policy: 0.0022\n  wait_policy_total: 93.7697\nupdate_model: 2.3025\n  weight_update: 0.0015\none_step: 0.0034\n  handle_policy_step: 105.2299\n    deserialize: 7.4926, stack: 0.6621, obs_to_device_normalize: 29.3540, forward: 38.4143, send_messages: 5.9522\n    prepare_outputs: 18.2651\n      to_cpu: 11.2702\n[2022-11-25 01:36:52,564][15762] Learner 0 profile tree view:\nmisc: 0.0024, prepare_batch: 8.0517\ntrain: 28.5942\n  epoch_init: 0.0037, minibatch_init: 0.0038, losses_postprocess: 0.1654, kl_divergence: 0.2093, after_optimizer: 12.5617\n  calculate_losses: 10.2242\n    losses_init: 0.0021, forward_head: 0.4746, bptt_initial: 7.5225, tail: 0.3432, advantages_returns: 0.0976, losses: 0.7113\n    bptt: 0.9616\n      bptt_forward_core: 0.9263\n  update: 5.0903\n    clip: 0.8172\n[2022-11-25 01:36:52,564][15762] RolloutWorker_w0 profile tree view:\nwait_for_trajectories: 0.0767, enqueue_policy_requests: 5.3569, env_step: 170.3642, overhead: 10.1567, complete_rollouts: 0.3764\nsave_policy_outputs: 6.6260\n  split_output_tensors: 3.0167\n[2022-11-25 01:36:52,564][15762] RolloutWorker_w11 profile tree view:\nwait_for_trajectories: 0.0816, enqueue_policy_requests: 5.5298, env_step: 169.3195, overhead: 10.2944, complete_rollouts: 0.3914\nsave_policy_outputs: 6.7380\n  split_output_tensors: 3.1037\n[2022-11-25 01:36:52,564][15762] Loop Runner_EvtLoop terminating...\n[2022-11-25 01:36:52,565][15762] Runner profile tree view:\nmain_loop: 217.4041\n[2022-11-25 01:36:52,565][15762] Collected {0: 4014080}, FPS: 18463.7\n</code></pre> <p>First thing to notice here: instead of a single report we have reports from all different types of components of our system: Batcher, InferenceWorker, Learner, RolloutWorker, Runner (main loop). There are 12 rollout workers but we see only 0<sup>th</sup> (first) and 11<sup>th</sup> (last) workers in the report - that's just to save space, reports from all other workers will be very similar.</p> <p>Total training time took 217 seconds at ~18400FPS (actual FPS reported during training was ~21000FPS, but this final number takes initialization time into account).</p> <p>Each individual report is a tree view of the time spent in different hotspots. For example, learner profile looks like this:</p> <pre><code>train: 28.5942\n  epoch_init: 0.0037, minibatch_init: 0.0038, losses_postprocess: 0.1654, kl_divergence: 0.2093, after_optimizer: 12.5617\n  calculate_losses: 10.2242\n    losses_init: 0.0021, forward_head: 0.4746, bptt_initial: 7.5225, tail: 0.3432, advantages_returns: 0.0976, losses: 0.7113\n    bptt: 0.9616\n      bptt_forward_core: 0.9263\n  update: 5.0903\n    clip: 0.8172\n</code></pre> <p><code>train</code> is the highest-level profiler context. On the next line we print all sub-profiles that don't have any sub-profiles of their own. In this case, <code>epoch_init</code>, <code>minibatch_init</code>, etc. After that, one by one, we print all sub-profiles that have sub-profiles of their own.</p> <p>Let's take a look at individual component reports:</p> <ul> <li>Runner (main loop) does not actually do any heavy work other than reporting summaries, so we can ignore it. It is here mostly to give us the total time from experiment start to finish.</li> <li>Batcher is responsible for batching trajectories from rollout workers and feeding them to the learner. In this case it only took 10 seconds and since it was done in parallel to all other work, we can ignore it for the most part, it's pretty fast.</li> <li>Learner's main hotspots took only 8 and 28 seconds. Again, considering that it was done in parallel to all other work, and the time is pretty insignificant compared to the total time of 217 seconds, we can safely say that it's not the bottleneck.</li> <li>InferenceWorker's overall time is 105 seconds, which is significant. We can see that the main hotspots are <code>forward</code> (actual forward pass) and <code>obs_to_device_normalize</code> (normalizing the observations and transferring them to GPU). In order to increase throughtput we might want to make our model faster (i.e. by making it smaller) or disable normalization (parameter <code>--normalize_input=False</code>, see config reference). Note however that both of these measures may hurt sample efficiency.</li> <li>RolloutWorkers that simulate the environment are the main culprits here. The majority of time is taken by <code>env_step</code> (stepping through the environment), ~170 seconds. Overall, we can say that this workload is heavily dominated by CPU-based simulation. If you're in a similar situation you might want to consider instrumenting your code deeper (i.e. using <code>Timing</code> or other tool) to measure hotspots in your environment and attempt to optimize it.</li> </ul>"},{"location":"07-advanced-topics/profiling/#notes-on-gpu-profiling","title":"Notes on GPU profiling","text":"<p>Profiling GPU-based workloads can be misleading because GPU kernels are asynchronous and sometimes we can see  a lot of time spent in sections after the ones we expect to be the hotspots. In the example above, the learner's <code>after_optimizer: 12.5617</code> is significantly longer than <code>update: 5.0903</code> where the actual backward pass happens.</p> <p>Thus one should not rely too heavily on timing your code for GPU profiling. Take a look at CUDA profiling, i.e. here is a Pytorch tutorial.</p> <p>Also check out this tutorial for some advanced RL profiling techniques.</p>"},{"location":"07-advanced-topics/profiling/#profiling-with-standard-python-profilers-cprofile-or-yappi","title":"Profiling with standard Python profilers (cProfile or yappi)","text":"<p>In most RL workloads in Sample Factory it can be difficult to use standard profiling tools because the full application consists of many processes and threads and in the author's experience standard tools struggle to organise traces from multiple processes into a single coherent report (if the reader knows of a good tool for this, please let the author know).</p> <p>However, using serial mode we can force Sample Factory to execute everything in one process! This can be very useful for finding bottlenecks in your environment implementation without the need for manual instrumentation. The following command will run the entire experiment in a single process:</p> <pre><code>python -m sf_examples.mujoco.train_mujoco --env=mujoco_ant --serial_mode=True --async_rl=False\n</code></pre> <p>Note that we enable synchronous RL mode as well, it's easier to debug this way and asynchronicity does not make sense when we're not using multiple processes.</p> <p>Moreover for some workloads it is actually optimal to run everything in a single process! This is true for GPU-accelerated environments such as IsaacGym or Brax. When env simulation, inference, and learning are all done on one GPU it is not necessarily beneficial to run these tasks in separate processes.</p> <p>In this case we can profile Sample Factory like any other Python application. For example, PyCharm has a nice visualizer for profiling results generated by <code>cProfile</code> or <code>yappi</code>. If we run training in IsaacGym in serial mode under PyCharm's profiler:</p> <pre><code>python -m sf_examples.isaacgym_examples.train_isaacgym --env=Ant --experiment=igeAnt\n</code></pre> <p>we get the following report which can be explored to find hotspots in different parts of the code: </p> <p></p>"},{"location":"07-advanced-topics/serial-mode/","title":"Serial Mode","text":"<p>Debugging an asynchronous system can be hard. In order to streamline debugging and development process, we provide a way to run all components of Sample Factory in a single process. Enable serial mode by setting <code>--serial_mode</code> to <code>True</code>.</p> <p>Serial regime is achieved via signal-slot mechanism. Components interact by sending and receiving signals. Thus they actually don't care whether they are running in the same process or in multiple processes. This allows us to put them all on the same event loop in the main process.</p>"},{"location":"07-advanced-topics/serial-mode/#applications","title":"Applications","text":"<p>The main use case for serial mode is debugging and development. If you're debugging your environment code, or any part of SF codebase, it is almost always easier to do it in serial mode.</p> <p>That said, for highly-vectorized GPU-accelerated environments it can be beneficial to run the whole system in serial mode, which is exactly what we do by default with IsaacGym. One advantage of serial mode is that we minimize the number of CUDA contexts and thus VRAM usage.</p>"},{"location":"07-advanced-topics/sync-async/","title":"Synchronous/Asynchronous RL","text":"<p>Since version 2.0 Sample Factory supports two training modes: synchronous and asynchronous. You can switch between them by setting <code>--async_rl=False</code> or <code>--async_rl=True</code> in the command line.</p>"},{"location":"07-advanced-topics/sync-async/#synchronous-rl","title":"Synchronous RL","text":"<p>In synchronous mode we collect trajectories from all environments until we have just enough data to fill a dataset (or training batch) on which the learner will perform one or more epochs of SGD.</p> <p>We operate synchronously: the system either collects the experience or trains the policy, not both at the same time. Rollout and Inference workers wait for the learner to finish the training before they start collecting more data.</p>"},{"location":"07-advanced-topics/sync-async/#asynchronous-rl","title":"Asynchronous RL","text":"<p>In asynchronous (default) mode we collect trajectories all the time and train the policy in the background. Once we have enough data to fill a dataset, we immediately start collecting new trajectories and will keep doing so until <code>--num_batches_to_accumulate</code> training batches are accumulated.</p>"},{"location":"07-advanced-topics/sync-async/#pros-and-cons","title":"Pros and Cons","text":"<p>There is no clear winner between the two modes. Try both regimes and see which one works better for you.</p> <ul> <li> <p>Async mode is often faster because we allow more computation to happen in parallel. As a tradeoff it introduces more policy-lag because some of the experience is collected by older versions of the policy (for example when we collect experience during training). So async mode enables faster training but might cost sample efficiency in some setups, for example  LSTM/GRU training is usually more susceptible to policy-lag than non-recurrent policies.</p> </li> <li> <p>Sync mode has more strict requirements for the system configuration because we're looking to collect the exact amount of data to fill a training batch. Example: we have <code>--num_workers=16</code>, <code>--num_envs_per_worker=8</code>, <code>--rollout=32</code>. This means in one iteration we collect 16 * 8 * 32 = 4096 steps of experience. Sync mode requires that training batch size is a multiple of 4096. This would work with <code>--batch_size=4096</code> and <code>--num_batches_per_epoch=1</code> or <code>--batch_size=2048</code> and <code>--num_batches_per_epoch=2</code>, but not with <code>--batch_size=512</code> and <code>--num_batches_per_epoch=3</code>. TLDR: sync mode provides less flexibility in the training configuration. In async mode we can do pretty much anything.</p> </li> <li> <p>For multi-policy and PBT setups we recommend using async mode. Async mode allows different policies to collect different amounts of experience per iteration, which allows us to use arbitrary mapping between agents and policies.</p> </li> </ul>"},{"location":"07-advanced-topics/sync-async/#visualization","title":"Visualization","text":"<p>The following animations may provide further insight into the difference between the two modes.</p> <ul> <li>Sync RL: https://www.youtube.com/watch?v=FHRG0lHVa54</li> <li>Async RL: https://www.youtube.com/watch?v=ML2WAQNpF90</li> </ul> <p>Note that the \"Sync RL\" animation is not 100% accurate to how SF works, we actually still do collect the experience asynchronously within the rollout, but then pause during training. \"Sync RL\" animation is closer to how a traditional RL implementation operates (e.g. OpenAI Baselines) and the comparison between the two shows why Sample Factory is often much faster.</p>"},{"location":"07-advanced-topics/sync-async/#vectorized-environments","title":"Vectorized environments","text":"<p>In GPU-accelerated environments like IsaacGym async mode does not provide a significant speedup because we do everything on the same device anyway. For these environments it is recommended to use sync mode for maximum sample efficiency.</p> <p>This animation demonstrates how synchronous learning works in a vectorized environment like IsaacGym: https://www.youtube.com/watch?v=EyUyDs4AA1Y</p>"},{"location":"08-miscellaneous/tests/","title":"Tests","text":"<p>To run unit tests install prereqiusites and execute the following command from the root of the repo: </p> <pre><code>pip install -e .[dev]\nmake test\n</code></pre> <p>Consider installing VizDoom for a more comprehensive set of tests.</p> <p>These tests are executed after each commit/PR by Github Actions. </p>"},{"location":"08-miscellaneous/tests/#test-ci-based-on-github-actions","title":"Test CI based on Github Actions","text":"<p>We build a test CI system based on Github Actions which will automatically run unit tests on different operating systems (currently Linux and macOS) with different python versions (currently 3.8, 3.9, 3.10) when you submit PRs or merge to the main branch.</p> <p>The test workflow is defined in <code>.github/workflows/test-ci.yml</code>.</p>"},{"location":"08-miscellaneous/v1-to-v2/","title":"v1 to v2 Migration","text":"<p>The repository has changed very significantly from the original Sample Factory v1, which makes it pretty much impossible to track all the changes.</p> <p>Perhaps the most obvious change that will affect everyone is that we removed generic entry points such as <code>train_appo.py</code> and <code>enjoy_appo.py</code>. As a consequence, now there's no difference between how \"custom\" and \"built-in\" environments are handled (custom envs are first-class citizens now). See train/enjoy scripts in <code>sf_examples</code> to see how to use the new API.</p> <p>If you have any custom code that registers custom environments of model architectures, please check Custom Environments and  Custom Models for the new API.</p> <p>Some configuration parameters were renamed:</p> <ul> <li><code>--ppo_epochs</code> -&gt; <code>--num_epochs</code></li> <li><code>--num_batches_per_iteration</code> -&gt; <code>--num_batches_per_epoch</code></li> <li><code>--num_minibatches_to_accumulate</code> -&gt; <code>--num_batches_to_accumulate</code> (also changed semantically, check the cfg reference)</li> </ul> <p><code>Runner</code> class we used to launch groups of experiments such as hyperparameter searches got renamed to <code>Launcher</code>. The name <code>Runner</code> now refers to an entirely different concept, a class that handles the main loop of the algorithm.</p> <p>Entities <code>ActorWorker</code> and <code>PolicyWorker</code> were renamed to <code>RolloutWorker</code> and <code>InferenceWorker</code> respectively.</p> <p>Due to the gravity of the changes it is difficult to provide a comprehensive migration guide. If you recently updated your codebase to use Sample Factory v2.0+, please consider sharing your experience and contribute to this guide! :)</p>"},{"location":"09-environment-integrations/atari/","title":"Atari","text":""},{"location":"09-environment-integrations/atari/#installation","title":"Installation","text":"<p>Install Sample Factory with Atari dependencies with PyPI:</p> <pre><code>pip install sample-factory[atari]\n</code></pre>"},{"location":"09-environment-integrations/atari/#running-experiments","title":"Running Experiments","text":"<p>Run Atari experiments with the scripts in <code>sf_examples.atari</code>.</p> <p>The default parameters have been chosen to match CleanRL's configuration (see reports below) and are not tuned for throughput. (see some better parameters at the end of the document).</p> <p>To train a model in the <code>BreakoutNoFrameskip-v4</code> environment:</p> <pre><code>python -m sf_examples.atari.train_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\"\n</code></pre> <p>To visualize the training results, use the <code>enjoy_atari</code> script:</p> <pre><code>python -m sf_examples.atari.enjoy_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\"\n</code></pre> <p>Multiple experiments can be run in parallel with the launcher module. <code>atari_envs</code> is an example launcher script that runs atari envs with 4 seeds. </p> <pre><code>python -m sample_factory.launcher.run --run=sf_examples.atari.experiments.atari_envs --backend=processes --max_parallel=8  --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1\n</code></pre>"},{"location":"09-environment-integrations/atari/#list-of-supported-environments","title":"List of Supported Environments","text":"<p>Specify the environment to run with the <code>--env</code> command line parameter. The following Atari v4 environments are supported out of the box. Various APPO models trained on Atari environments are uploaded to the HuggingFace Hub. The models have all been trained for 2 billion steps with 3 seeds per experiment. Videos of the agents after training can be found on the HuggingFace Hub.</p> Atari Command Line Parameter Atari Environment name Model Checkpooints atari_alien AlienNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_amidar AmidarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_assault AssaultNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asterix AsterixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_asteroid AsteroidsNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_atlantis AtlantisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bankheist BankHeistNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_battlezone BattleZoneNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_beamrider BeamRiderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_berzerk BerzerkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_bowling BowlingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_boxing BoxingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_breakout BreakoutNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_centipede CentipedeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_choppercommand ChopperCommandNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_crazyclimber CrazyClimberNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_defender DefenderNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_demonattack DemonAttackNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_doubledunk DoubleDunkNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_enduro EnduroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_fishingderby FishingDerbyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_freeway FreewayNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_frostbite FrostbiteNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gopher GopherNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_gravitar GravitarNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_hero HeroNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_icehockey IceHockeyNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_jamesbond JamesbondNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kangaroo KangarooNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_krull KrullNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_kongfumaster KungFuMasterNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_montezuma MontezumaRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_mspacman MsPacmanNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_namethisgame NameThisGameNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_phoenix PhoenixNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pitfall PitfallNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_pong PongNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_privateye PrivateEyeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_qbert QbertNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_riverraid RiverraidNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_roadrunner RoadRunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_robotank RobotankNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_seaquest SeaquestNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_skiing SkiingNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_solaris SolarisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_spaceinvaders SpaceInvadersNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_stargunner StarGunnerNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_surround SurroundNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tennis TennisNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_timepilot TimePilotNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_tutankham TutankhamNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_upndown UpNDownNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_venture VentureNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_videopinball VideoPinballNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_wizardofwor WizardOfWorNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_yarsrevenge YarsRevengeNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints atari_zaxxon ZaxxonNoFrameskip-v4 \ud83e\udd17 Hub Atari-2B checkpoints"},{"location":"09-environment-integrations/atari/#reports","title":"Reports","text":"<ul> <li>Sample Factory was benchmarked on Atari against CleanRL and Baselines. Sample Factory was able to achieve similar sample efficiency as CleanRL and Baselines using the same parameters.<ul> <li>https://wandb.ai/wmfrank/atari-benchmark/reports/Atari-Sample-Factory2-Baselines-CleanRL--VmlldzoyMzEyNjIw</li> </ul> </li> </ul>"},{"location":"09-environment-integrations/atari/#better-parameters-more-envs-double-buffering-async-learning","title":"Better parameters (more envs, double buffering, async learning)","text":"<pre><code>--experiment=breakout_faster\n--env=atari_breakout\n--summaries_use_frameskip=False\n--num_workers=16\n--num_envs_per_worker=8\n--worker_num_splits=2\n--train_for_env_steps=100000000\n--rollout=32\n--normalize_input=True\n--normalize_returns=True\n--serial_mode=False\n--async_rl=True\n--batch_size=1024\n--wandb_user=&lt;user&gt;\n--wandb_project=sf2_atari_breakout\n--wandb_group=breakout_w16v8r32\n--with_wandb=True\n</code></pre> <p>Report: https://wandb.ai/apetrenko/sf2_atari_breakout/reports/sf2-breakout-w16v8r32--Vmlldzo0MjM1MTQ4</p>"},{"location":"09-environment-integrations/brax/","title":"Brax","text":""},{"location":"09-environment-integrations/brax/#installation","title":"Installation","text":"<p>Installing Brax with CUDA acceleration can be a bit tricky. There are some notes here: https://github.com/google/jax#pip-installation-gpu-cuda.</p> <p>I had the best luck with the following steps:</p> <pre><code># Create a Conda environment or use your existing one\nconda create --name sf_brax python=3.9\n\n# Activate the environment\nconda activate sf_brax\n\n# cuda-nvcc seems to be necessary, and the order of conda repos matters\nconda install cudatoolkit cuda-nvcc -c conda-forge -c nvidia\n\n# Install Jax/Jaxlib from a custom repo\npip install  --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n# Install Brax\npip install brax\n</code></pre> <p>Then follow general instructions to install Sample Factory if you need to.</p>"},{"location":"09-environment-integrations/brax/#running-experiments","title":"Running Experiments","text":"<pre><code># to avoid OOM issues it is advised to disable vram preallocation (might not be necessary)\nexport XLA_PYTHON_CLIENT_PREALLOCATE=false\n\n# train for 100M steps with default hyperparameters\npython -m sf_examples.brax.train_brax --env=ant --experiment=ant_brax\n\n# evaluate the agent\npython -m sf_examples.brax.enjoy_brax --env=ant --experiment=ant_brax\n\n# Brax software renderer is quite slow, so you can render a video offscreen instead of visualizing it in a window\n# Video will be saved to the experiment directory\npython -m sf_examples.brax.enjoy_brax --env=ant --experiment=ant_brax --save_video --video_name=ant\n</code></pre>"},{"location":"09-environment-integrations/brax/#results","title":"Results","text":""},{"location":"09-environment-integrations/brax/#reports","title":"Reports","text":"<p>The following reports were created after running a launcher script on a Slurm cluster with the following command:</p> <pre><code>python -m sample_factory.launcher.run --run=sf_examples.brax.experiments.brax_basic_envs --backend=slurm --slurm_workdir=./slurm_brax --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=8 --slurm_sbatch_template=./sf_examples/brax/experiments/sbatch_timeout_brax.sh --pause_between=0 --slurm_print_only=False\n</code></pre> <ol> <li>ant: https://api.wandb.ai/report/apetrenko/ji9jygss</li> <li>humanoid: https://api.wandb.ai/report/apetrenko/m520i16m</li> <li>halfcheetah: https://api.wandb.ai/report/apetrenko/7xlp3hh8</li> <li>walker2d: https://api.wandb.ai/report/apetrenko/pvb9d11c</li> </ol>"},{"location":"09-environment-integrations/brax/#models","title":"Models","text":"Environment HuggingFace Hub Models Evaluation Metrics ant https://huggingface.co/apetrenko/sample_factory_brax_ant 12565.17 \u00b1 3350.51 humanoid https://huggingface.co/apetrenko/sample_factory_brax_humanoid 33847.53 \u00b1 6327.36 halfcheetah https://huggingface.co/apetrenko/sample_factory_brax_halfcheetah 22298.35 \u00b1 1882.48 walker2d https://huggingface.co/apetrenko/sample_factory_brax_walker2d 5459.17 \u00b1 2198.74 <p>Example command line used to generate a HuggingFace Hub model:</p> <pre><code>python -m sf_examples.brax.enjoy_brax \\\n  --env=humanoid --experiment=02_v083_brax_basic_benchmark_see_2322090_env_humanoid_u.rnn_False_n.epo_5 \\\n  --train_dir=/home/alex/all/projects/sf2/train_dir/v083_brax_basic_benchmark/v083_brax_basic_benchmark_slurm \\\n  --save_video --video_frames=500 --max_num_episodes=500 \\\n  --enjoy_script=sf_examples.brax.enjoy_brax --train_script=sf_examples.brax.train_brax \\\n  --push_to_hub --hf_repository=apetrenko/sample_factory_brax_humanoid --brax_render_res=320 --load_checkpoint_kind=best\n</code></pre>"},{"location":"09-environment-integrations/brax/#videos","title":"Videos","text":""},{"location":"09-environment-integrations/brax/#ant-environment","title":"Ant Environment","text":""},{"location":"09-environment-integrations/brax/#humanoid-environment","title":"Humanoid Environment","text":""},{"location":"09-environment-integrations/dmlab/","title":"DeepMind Lab","text":""},{"location":"09-environment-integrations/dmlab/#installation","title":"Installation","text":"<p>Installation DeepMind Lab can be time consuming. If you are on a Linux system, we provide a prebuilt wheel.</p> <ul> <li>Either <code>pip install deepmind_lab-1.0-py3-none-any.whl</code></li> <li>Or alternatively, DMLab can be compiled from source by following the instructions on the DMLab Github.</li> <li><code>pip install dm_env</code></li> <li>To train on DMLab-30 you will need <code>brady_konkle_oliva2008</code> dataset.</li> <li>To significantly speed up training on DMLab-30 consider downloading our dataset of pre-generated environment layouts (see paper for details). Command lines for running experiments with these datasets are provided in the sections below.</li> </ul>"},{"location":"09-environment-integrations/dmlab/#running-experiments","title":"Running Experiments","text":"<p>Run DMLab experiments with the scripts in <code>sf_examples.dmlab</code>. </p> <p>Example of training in the DMLab watermaze environment for 1B environment steps</p> <pre><code>python -m sf_examples.dmlab.train_dmlab --env=dmlab_watermaze --train_for_env_steps=1000000000 --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=1 --experiment=dmlab_watermaze_resnet_w90_v12 --set_workers_cpu_affinity=True --max_policy_lag=35  --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache\n</code></pre> <p>DMLab-30 run on a 36-core server with 4 GPUs using Population-Based Training with 4 agents:</p> <pre><code>python -m sf_examples.dmlab.train_dmlab --env=dmlab_30 --train_for_env_steps=10000000000 --gamma=0.99 --use_rnn=True --num_workers=90 --num_envs_per_worker=12 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=2048 --benchmark=False --max_grad_norm=0.0 --dmlab_renderer=software --decorrelate_experience_max_seconds=120 --encoder_conv_architecture=resnet_impala --encoder_conv_mlp_layers=512 --nonlinearity=relu --rnn_type=lstm --dmlab_extended_action_set=True --num_policies=4 --pbt_replace_reward_gap=0.05 --pbt_replace_reward_gap_absolute=5.0 --pbt_period_env_steps=10000000 --pbt_start_mutation=100000000 --with_pbt=True --experiment=dmlab_30_resnet_4pbt_w90_v12 --dmlab_one_task_per_worker=True --set_workers_cpu_affinity=True --max_policy_lag=35 --pbt_target_objective=dmlab_target_objective --dmlab30_dataset=~/datasets/brady_konkle_oliva2008 --dmlab_use_level_cache=True --dmlab_level_cache_path=/home/user/dmlab_cache\n</code></pre>"},{"location":"09-environment-integrations/dmlab/#models","title":"Models","text":"DMLab Command Line Parameter DMLab Environment name Model Checkpooints dmlab_30 DMLab-30 \ud83e\udd17 Hub DMLab30 checkpoints"},{"location":"09-environment-integrations/envpool/","title":"Envpool","text":""},{"location":"09-environment-integrations/envpool/#installation","title":"Installation","text":"<p>Install Sample-Factory with Envpool dependencies with PyPI:</p> <pre><code>pip install sample-factory[atari,envpool]\npip install sample-factory[mujoco,envpool]\n</code></pre>"},{"location":"09-environment-integrations/envpool/#running-experiments","title":"Running Experiments","text":"<p>EnvPool is a C++-based batched environment pool with pybind11 and thread pool. It has high performance (~1M raw FPS with Atari games, ~3M raw FPS with Mujoco simulator).</p> <p>We provide examples for envpool for Atari and Mujoco environments. The default parameters provide reasonable training speed, but can be tuning based on your machine configuration to achieve higher throughput.</p> <p>To train a model with envpool in the <code>BreakoutNoFrameskip-v4</code> environment:</p> <pre><code>python -m sf_examples.envpool.atari.train_envpool_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\"\n</code></pre> <p>To visualize the training results, use the <code>enjoy_envpool_atari</code> script:</p> <pre><code>python -m sf_examples.envpool.atari.enjoy_envpool_atari --algo=APPO --env=atari_breakout --experiment=\"Experiment Name\"\n</code></pre> <p>Multiple experiments can be run in parallel with the launcher module. <code>atari_envs</code> is an example launcher script that runs atari envs with 4 seeds. </p> <pre><code>python -m sample_factory.launcher.run --run=sf_examples.envpool.atari.experiments.atari_envs --backend=processes --max_parallel=8  --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1\n</code></pre>"},{"location":"09-environment-integrations/envpool/#reports","title":"Reports","text":"<ol> <li>Sample-Factory's envpool environments were benchmarked against RL-Games using the same parameters. Sample-Factory achieved the same sample efficiency and wall time in the MuJoCo Ant environment. Additionally, using Envpool for this environment decrease the wall time compared to the default SF parameters without using Envpool.<ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/Envpool-Sample-Factory-vs-RL-Games-in-MuJoCo--VmlldzozMTEyNjk3</li> </ul> </li> </ol>"},{"location":"09-environment-integrations/isaacgym/","title":"IsaacGym","text":""},{"location":"09-environment-integrations/isaacgym/#installation","title":"Installation","text":"<p>Install IsaacGym from NVIDIA at https://developer.nvidia.com/isaac-gym. Installation instructions can be found in the package's docs folder. Python 3.8 is compatable with both IsaacGym and Sample-Factory</p> <p>Install IsaacGymEnvs from https://github.com/NVIDIA-Omniverse/IsaacGymEnvs. </p>"},{"location":"09-environment-integrations/isaacgym/#running-experiments","title":"Running Experiments","text":"<p>Run IsaacGym experiments using scripts from the <code>sf_examples.isaacgym_examples</code> folder. Currently, we support the AllegroHand, Ant, Anymal, AnymalTerrain, BallBalance, Cartpole , Humanoid, and ShadowHand  environments out of the box, and more environments can be added in <code>train_isaacgym.py</code>.</p> <p>To run an experiment in the Ant environment: <pre><code>python -m sf_examples.isaacgym_examples.train_isaacgym --actor_worker_gpus 0 --env=Ant --train_for_env_steps=100000000  --experiment=isaacgym_ant\n</code></pre></p> <p>Multiple experiments can be run in parallel using the experiment launcher. See the <code>experiments</code> folder in <code>sf_examples.isaacgym_examples</code> for examples. To run multiple Ant and Humanoid experiments, run: <pre><code>python -m sample_factory.launcher.run --run=sf_examples.isaacgym_examples.experiments.isaacgym_basic_envs --backend=processes --max_parallel=2 --experiments_per_gpu=2 --num_gpus=1\n</code></pre></p>"},{"location":"09-environment-integrations/isaacgym/#results","title":"Results","text":""},{"location":"09-environment-integrations/isaacgym/#reports","title":"Reports","text":"<ol> <li> <p>We tested the IsaacGym Ant and Humanoid environments with and without recurrence. When using an RNN and recurrence, the Ant and Humanoid environments see an improvement in sample efficiency. However, there is a decrease in wall time efficiency.</p> <ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/IsaacGym-Ant-and-Humanoid--VmlldzozMDUxNTky</li> </ul> </li> <li> <p>The AllegroHand environment was tested with and without return normalization. Return normalization is essential to this environment as it improved the performance by around 200%</p> <ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/IsaacGym-AllegroHand--VmlldzozMDUxNjA2</li> </ul> </li> </ol>"},{"location":"09-environment-integrations/isaacgym/#models","title":"Models","text":"Environment HuggingFace Hub Models Evaluation Metrics Ant https://huggingface.co/andrewzhang505/isaacgym_ant 11830.10 \u00b1 875.26 Humanoid https://huggingface.co/andrewzhang505/isaacgym_humanoid 8839.07 \u00b1 407.26 AllegroHand https://huggingface.co/andrewzhang505/isaacgym_allegrohand 3608.18 \u00b1 1062.94"},{"location":"09-environment-integrations/isaacgym/#videos","title":"Videos","text":""},{"location":"09-environment-integrations/isaacgym/#ant-environment","title":"Ant Environment","text":""},{"location":"09-environment-integrations/isaacgym/#humanoid-environment","title":"Humanoid Environment","text":""},{"location":"09-environment-integrations/isaacgym/#allegrohand-environment","title":"AllegroHand Environment","text":""},{"location":"09-environment-integrations/megaverse/","title":"Megaverse","text":"<p>Megaverse is a dedicated high-throughput RL environment with batched GPU rendering.</p> <p>This document demonstrates an example of external integration, i.e. another project using Sample Factory as a library. Very likely this is going to be the most common integration scenario.</p>"},{"location":"09-environment-integrations/megaverse/#installation","title":"Installation","text":"<p>Install Megaverse according to the readme of the repo Megaverse. Further instructions assume that you are in a Python (or Conda) environment with a working Megaverse installation.</p>"},{"location":"09-environment-integrations/megaverse/#running-experiments","title":"Running Experiments","text":"<p>Run Megaverse experiments with the scripts in <code>megaverse_rl</code>.</p> <p>To train a model in the <code>TowerBuilding</code> environment:</p> <pre><code>python -m megaverse_rl.train_megaverse --train_for_seconds=360000000 --train_for_env_steps=2000000000 --algo=APPO --gamma=0.997 --use_rnn=True --rnn_num_layers=2 --num_workers=12 --num_envs_per_worker=2 --num_epochs=1 --rollout=32 --recurrence=32 --batch_size=4096 --actor_worker_gpus 0 --env_gpu_observations=False --num_policies=1 --with_pbt=False --max_grad_norm=0.0 --exploration_loss=symmetric_kl --exploration_loss_coeff=0.001 --megaverse_num_simulation_threads=1 --megaverse_num_envs_per_instance=32 --megaverse_num_agents_per_env=1 --megaverse_use_vulkan=True --policy_workers_per_policy=2 --reward_clip=30 --env=TowerBuilding --experiment=TowerBuilding\n</code></pre> <p>To visualize the training results, use the <code>enjoy_megaverse</code> script:</p> <pre><code>python -m megaverse_rl.enjoy_megaverse --algo=APPO --env=TowerBuilding --experiment=TowerBuilding --megaverse_num_envs_per_instance=1 --fps=20 --megaverse_use_vulkan=True\n</code></pre> <p>Multiple experiments can be run in parallel with the launcher module. <code>megaverse_envs</code> is an example launcher script that runs megaverse envs with 5 seeds. </p> <pre><code>python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=processes --max_parallel=2  --pause_between=1 --experiments_per_gpu=2 --num_gpus=1\n</code></pre> <p>Or you could run experiments on slurm:</p> <pre><code>python -m sample_factory.launcher.run --run=megaverse_rl.runs.single_agent --backend=slurm --slurm_workdir=./slurm_megaverse --experiment_suffix=slurm --slurm_gpus_per_job=1 --slurm_cpus_per_gpu=16 --slurm_sbatch_template=./sample_factory/launcher/slurm/sbatch_timeout.sh --pause_between=1 --slurm_print_only=False\n</code></pre>"},{"location":"09-environment-integrations/megaverse/#results","title":"Results","text":""},{"location":"09-environment-integrations/megaverse/#reports","title":"Reports","text":"<ul> <li>We trained models in the <code>TowerBuilding</code> environment in SF2 with single agent per env.<ul> <li>https://wandb.ai/wmfrank/megaverse-benchmark/reports/Megaverse-trained-Sample-Factory--VmlldzoyNTAxMDUz</li> </ul> </li> </ul>"},{"location":"09-environment-integrations/megaverse/#models","title":"Models","text":"<p>An example APPO model trained on Megaverse environments is uploaded to the HuggingFace Hub. The models have all been trained for 2G steps.</p> Environment HuggingFace Hub Models TowerBuilding https://huggingface.co/wmFrank/sample-factory-2-megaverse"},{"location":"09-environment-integrations/megaverse/#tower-building-with-single-agent","title":"Tower Building with single agent","text":""},{"location":"09-environment-integrations/mujoco/","title":"MuJoCo","text":""},{"location":"09-environment-integrations/mujoco/#installation","title":"Installation","text":"<p>Install Sample Factory with MuJoCo dependencies with PyPI:</p> <pre><code>pip install sample-factory[mujoco]\n</code></pre>"},{"location":"09-environment-integrations/mujoco/#running-experiments","title":"Running Experiments","text":"<p>Run MuJoCo experiments with the scripts in <code>sf_examples.mujoco</code>. The default parameters have been chosen to match CleanRL's results in the report below (please note that we can achieve even faster training on a multi-core machine with more optimal parameters).</p> <p>To train a model in the <code>Ant-v4</code> environment:</p> <pre><code>python -m sf_examples.mujoco.train_mujoco --env=mujoco_ant --experiment=&lt;experiment_name&gt;\n</code></pre> <p>To visualize the training results, use the <code>enjoy_mujoco</code> script:</p> <pre><code>python -m sf_examples.mujoco.enjoy_mujoco --env=mujoco_ant --experiment=&lt;experiment_name&gt;\n</code></pre> <p>If you're having issues with the Mujoco viewer in a Unix/Linux environment with Conda, try running the following before executing the <code>enjoy_mujoco</code> script:</p> <pre><code>export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6\npython -m sf_examples.mujoco.enjoy_mujoco ...\n</code></pre> <p>Multiple experiments can be run in parallel with the launcher module. <code>mujoco_all_envs</code> is an example launcher script that runs all mujoco envs with 10 seeds. </p> <pre><code>python -m sample_factory.launcher.run --run=sf_examples.mujoco.experiments.mujoco_all_envs --backend=processes --max_parallel=4  --pause_between=1 --experiments_per_gpu=10000 --num_gpus=1 --experiment_suffix=0\n</code></pre>"},{"location":"09-environment-integrations/mujoco/#list-of-supported-environments","title":"List of Supported Environments","text":"<p>Specify the environment to run with the <code>--env</code> command line parameter. The following MuJoCo v4 environments are supported out of the box, and more environments can be added as needed in <code>sf_examples.mujoco.mujoco.mujoco_utils</code></p> MuJoCo Environment Name Sample Factory Command Line Parameter Ant-v4 mujoco_ant HalfCheetah-v4 mujoco_halfcheetah Hopper-v4 mujoco_hopper Humanoid-v4 mujoco_humanoid Walker2d-v4 mujoco_walker InvertedDoublePendulum-v4 mujoco_doublependulum InvertedPendulum-v4 mujoco_pendulum Reacher-v4 mujoco_reacher Swimmer-v4 mujoco_swimmer"},{"location":"09-environment-integrations/mujoco/#results","title":"Results","text":""},{"location":"09-environment-integrations/mujoco/#reports","title":"Reports","text":"<ol> <li> <p>Sample Factory was benchmarked on MuJoCo against CleanRL. Sample-Factory was able to achieve similar sample efficiency as CleanRL using the same parameters.</p> <ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-w-o-EnvPool--VmlldzoyMjMyMTQ0</li> </ul> </li> <li> <p>Sample Factory can run experiments synchronously or asynchronously, with asynchronous execution usually having worse sample efficiency but runs faster. MuJoCo's environments were compared using the two modes in Sample-Factory</p> <ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Synchronous-vs-Asynchronous--VmlldzoyMzEzNDUz</li> </ul> </li> <li> <p>Sample Factory comparison with CleanRL in terms of wall time. Both experiments are run on a 16 core machine with 1 GPU. Sample-Factory was able to complete 10M samples 5 times as fast as CleanRL</p> <ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/MuJoCo-Sample-Factory-vs-CleanRL-Wall-Time--VmlldzoyMzg2MDA3</li> </ul> </li> </ol>"},{"location":"09-environment-integrations/mujoco/#models","title":"Models","text":"<p>Various APPO models trained on MuJoCo environments are uploaded to the HuggingFace Hub. The models have all been trained for 10M steps. Videos of the agents after training can be found on the HuggingFace Hub.</p> <p>The models below are the best models from the experiment against CleanRL above. The evaluation metrics here are obtained by running the model 10 times.</p> Environment HuggingFace Hub Models Evaluation Metrics Ant-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-ant 5876.09 \u00b1 166.99 HalfCheetah-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-halfcheetah 6262.56 \u00b1 67.29 Humanoid-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-humanoid 5439.48 \u00b1 1314.24 Walker2d-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-walker 5487.74 \u00b1 48.96 Hopper-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-hopper 2793.44 \u00b1 642.58 InvertedDoublePendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-doublependulum 9350.13 \u00b1 1.31 InvertedPendulum-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-pendulum 1000.00 \u00b1 0.00 Reacher-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-reacher -4.53 \u00b1 1.79 Swimmer-v4 https://huggingface.co/andrewzhang505/sample-factory-2-mujoco-swimmer 117.28 \u00b1 2.91"},{"location":"09-environment-integrations/mujoco/#videos","title":"Videos","text":"<p>Below are some video examples of agents in various MuJoCo envioronments. Videos for all environments can be found in the HuggingFace Hub pages linked above.</p>"},{"location":"09-environment-integrations/mujoco/#halfcheetah-v4","title":"HalfCheetah-v4","text":""},{"location":"09-environment-integrations/mujoco/#ant-v4","title":"Ant-v4","text":""},{"location":"09-environment-integrations/mujoco/#inverteddoublependulum-v4","title":"InvertedDoublePendulum-v4","text":""},{"location":"09-environment-integrations/nethack/","title":"NetHack","text":""},{"location":"09-environment-integrations/nethack/#installation","title":"Installation","text":"<p>Works in <code>Python 3.10</code>. Higher versions have problems with building NLE.</p> <p>To install NetHack, you need nle and its dependencies.</p> <pre><code># nle dependencies\napt-get install build-essential python3-dev python3-pip python3-numpy autoconf libtool pkg-config libbz2-dev\nconda install cmake flex bison lit\n\n# install nle locally and modify it to enable seeding and handle rendering with gymnasium\ngit clone https://github.com/facebookresearch/nle.git nle &amp;&amp; cd nle \\\n&amp;&amp; git checkout v0.9.0 &amp;&amp; git submodule init &amp;&amp; git submodule update --recursive \\\n&amp;&amp; sed '/#define NLE_ALLOW_SEEDING 1/i#define NLE_ALLOW_SEEDING 1' include/nleobs.h -i \\\n&amp;&amp; sed '/self\\.nethack\\.set_initial_seeds = f/d' nle/env/tasks.py -i \\\n&amp;&amp; sed '/self\\.nethack\\.set_current_seeds = f/d' nle/env/tasks.py -i \\\n&amp;&amp; sed '/self\\.nethack\\.get_current_seeds = f/d' nle/env/tasks.py -i \\\n&amp;&amp; sed '/def seed(self, core=None, disp=None, reseed=True):/d' nle/env/tasks.py -i \\\n&amp;&amp; sed '/raise RuntimeError(\"NetHackChallenge doesn.t allow seed changes\")/d' nle/env/tasks.py -i \\\n&amp;&amp; python setup.py install &amp;&amp; cd .. \n\n# install sample factory with nethack extras\npip install -e .[nethack]\nconda install -c conda-forge pybind11\npip install -e sf_examples/nethack/nethack_render_utils\n</code></pre>"},{"location":"09-environment-integrations/nethack/#running-experiments","title":"Running Experiments","text":"<p>Run NetHack experiments with the scripts in <code>sf_examples.nethack</code>. The default parameters have been chosen to match dungeons &amp; data which is based on nle sample factory baseline. By moving from D&amp;D to sample factory we've managed to increase the APPO score from 2k to 2.8k.</p> <p>To train a model in the <code>nethack_challenge</code> environment:</p> <pre><code>python -m sf_examples.nethack.train_nethack \\\n    --env=nethack_challenge \\\n    --batch_size=4096 \\\n    --num_workers=16 \\\n    --num_envs_per_worker=32 \\\n    --worker_num_splits=2 \\\n    --rollout=32 \\\n    --character=mon-hum-neu-mal \\\n    --model=ChaoticDwarvenGPT5 \\\n    --rnn_size=512 \\\n    --experiment=nethack_monk\n</code></pre> <p>To visualize the training results, use the <code>enjoy_nethack</code> script:</p> <pre><code>python -m sf_examples.nethack.enjoy_nethack --env=nethack_challenge --character=mon-hum-neu-mal --experiment=nethack_monk\n</code></pre> <p>Additionally it's possible to use an alternative <code>fast_eval_nethack</code> script which is much faster</p> <pre><code>python -m sf_examples.nethack.fast_eval_nethack --env=nethack_challenge --sample_env_episodes=128 --num_workers=16 --num_envs_per_worker=2 --character=mon-hum-neu-mal --experiment=nethack_monk \n</code></pre>"},{"location":"09-environment-integrations/nethack/#list-of-supported-environments","title":"List of Supported Environments","text":"<ul> <li>nethack_staircase</li> <li>nethack_score</li> <li>nethack_pet</li> <li>nethack_oracle</li> <li>nethack_gold</li> <li>nethack_eat</li> <li>nethack_scout</li> <li>nethack_challenge</li> </ul>"},{"location":"09-environment-integrations/nethack/#results","title":"Results","text":""},{"location":"09-environment-integrations/nethack/#reports","title":"Reports","text":"<ol> <li>Sample Factory was benchmarked on <code>nethack_challenge</code> against Dungeons and Data. Sample-Factory was able to achieve similar sample efficiency as D&amp;D using the same parameters and get better running returns (2.8k vs 2k). Training was done on <code>nethack_challenge</code> with human-monk character for 2B env steps.<ul> <li>https://api.wandb.ai/links/bartekcupial/w69fid1w</li> </ul> </li> </ol>"},{"location":"09-environment-integrations/nethack/#models","title":"Models","text":"<p>Sample Factory APPO model trained on <code>nethack_challenge</code> environment is uploaded to the HuggingFace Hub. The model have been trained for 2B steps.</p> <p>The model below is the best model from the experiment against Dungeons and Data above. The evaluation metrics here are obtained by running the model 1024 times. </p> <p>Model card: https://huggingface.co/LLParallax/sample_factory_human_monk Evaluation results: <pre><code>{\n    \"reward/reward\": 3245.3828125,\n    \"reward/reward_min\": 20.0,\n    \"reward/reward_max\": 18384.0,\n    \"len/len\": 2370.4560546875,\n    \"len/len_min\": 27.0,\n    \"len/len_max\": 21374.0,\n    \"policy_stats/avg_score\": 3245.4716796875,\n    \"policy_stats/avg_turns\": 14693.970703125,\n    \"policy_stats/avg_dlvl\": 1.13671875,\n    \"policy_stats/avg_max_hitpoints\": 46.42578125,\n    \"policy_stats/avg_max_energy\": 34.00390625,\n    \"policy_stats/avg_armor_class\": 4.68359375,\n    \"policy_stats/avg_experience_level\": 6.13671875,\n    \"policy_stats/avg_experience_points\": 663.375,\n    \"policy_stats/avg_eating_score\": 14063.2587890625,\n    \"policy_stats/avg_gold_score\": 76.033203125,\n    \"policy_stats/avg_scout_score\": 499.0478515625,\n    \"policy_stats/avg_sokobanfillpit_score\": 0.0,\n    \"policy_stats/avg_staircase_pet_score\": 0.005859375,\n    \"policy_stats/avg_staircase_score\": 4.9970703125,\n    \"policy_stats/avg_episode_number\": 1.5,\n    \"policy_stats/avg_true_objective\": 3245.3828125,\n    \"policy_stats/avg_true_objective_min\": 20.0,\n    \"policy_stats/avg_true_objective_max\": 18384.0\n}\n</code></pre></p>"},{"location":"09-environment-integrations/swarm-rl/","title":"Quad-Swarm-RL Integrations","text":""},{"location":"09-environment-integrations/swarm-rl/#installation","title":"Installation","text":"<p>Clone https://github.com/Zhehui-Huang/quad-swarm-rl into your home directory</p> <p>Install dependencies in your conda environment <pre><code>cd ~/quad-swarm-rl\npip install -e .\n</code></pre></p> <p>Note: if you have any error with bezier, run: <pre><code>BEZIER_NO_EXTENSION=true pip install bezier==2020.5.19\npip install -e .\n</code></pre></p>"},{"location":"09-environment-integrations/swarm-rl/#running-experiments","title":"Running Experiments","text":"<p>The environments can be run from the <code>quad_swarm_rl</code> folder in the downloaded <code>quad-swarm-rl</code> directory instead of from <code>sample-factory</code> directly. </p> <p>Experiments can be run with the <code>train</code> script and viewed with the <code>enjoy</code> script. If you are running custom experiments, it is recommended to use the <code>quad_multi_mix_baseline</code> runner script and make any modifications as needed. See <code>sf2_single_drone</code> and <code>sf2_multi_drone</code> runner scripts for an examples.</p> <p>The quadrotor environments have many unique parameters that can be found in <code>quadrotor_params.py</code>. Some relevant params for rendering results include <code>--quads_view_mode</code> which can be set to local or global for viewing multi-drone experiments, and <code>--quads_mode</code> which determines which scenario(s) to train on, with <code>mix</code> using all scenarios.</p>"},{"location":"09-environment-integrations/swarm-rl/#results","title":"Results","text":""},{"location":"09-environment-integrations/swarm-rl/#reports","title":"Reports","text":"<ol> <li>Comparison using a single drone between normalized (input and return normalization) and un-normalized experiments. Normalization helped the drones learn in around half the number of steps.<ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/Quad-Swarm-RL--VmlldzoyMzU1ODQ1</li> </ul> </li> <li>Experiments with 8 drones in scenarios with and without obstacles. All experiments used input and return normalization. Research and development are still being done on multi-drone scenarios to reduce the number of collisions.<ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/Quad-Swarm-RL-Multi-Drone--VmlldzoyNDkwNDQ0</li> </ul> </li> </ol>"},{"location":"09-environment-integrations/swarm-rl/#models","title":"Models","text":"Description HuggingFace Hub Models Evaluation Metrics Single drone with normalization https://huggingface.co/andrewzhang505/quad-swarm-single-drone-sf2 0.03 \u00b1 1.86 Multi drone without obstacles https://huggingface.co/andrewzhang505/quad-swarm-rl-multi-drone-no-obstacles -0.40 \u00b1 4.47 Multi drone with obstacles https://huggingface.co/andrewzhang505/quad-swarm-rl-multi-drone-obstacles -2.84 \u00b1 3.71"},{"location":"09-environment-integrations/swarm-rl/#videos","title":"Videos","text":"<p>Single drone with normalization flying between dynamic goals.</p> <p> </p>"},{"location":"09-environment-integrations/vizdoom/","title":"VizDoom","text":""},{"location":"09-environment-integrations/vizdoom/#installation","title":"Installation","text":"<p>To install VizDoom just follow system setup instructions from the original repository (VizDoom linux_deps), after which the latest VizDoom can be installed from PyPI:  <code>pip install vizdoom</code></p>"},{"location":"09-environment-integrations/vizdoom/#running-experiments","title":"Running Experiments","text":"<p>Run experiments with the scripts in <code>sf_examples.vizdoom</code>.</p> <p>Train for 4B env steps (also can be stopped at any time with Ctrl+C and resumed by using the same cmd). This is more or less optimal training setup for a 10-core machine.</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --batch_size=2048 --wide_aspect_ratio=False --num_workers=20 --num_envs_per_worker=20 --num_policies=1  --experiment=doom_battle_w20_v20\n</code></pre> <p>Run at any point to visualize the experiment:</p> <pre><code>python -m sf_examples.vizdoom.enjoy_vizdoom --env=doom_battle --algo=APPO --experiment=doom_battle_w20_v20\n</code></pre> <p>Launcher scripts are also provided in <code>sf_examples.vizdoom.experiments</code> to run experiments in parallel or on slurm.</p>"},{"location":"09-environment-integrations/vizdoom/#reproducing-paper-results","title":"Reproducing Paper Results","text":"<p>Train on one of the 6 \"basic\" VizDoom environments:</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --train_for_env_steps=500000000 --algo=APPO --env=doom_my_way_home --env_frameskip=4 --use_rnn=True --num_workers=36 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --experiment=doom_basic_envs\n</code></pre> <p>Doom \"battle\" and \"battle2\" environments, 36-core server (72 logical cores) with 4 GPUs: <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_battle --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle\npython -m sf_examples.vizdoom.train_vizdoom --env=doom_battle2 --train_for_env_steps=4000000000 --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=8 --num_policies=1 --batch_size=2048 --wide_aspect_ratio=False --max_grad_norm=0.0 --experiment=doom_battle_2\n</code></pre></p> <p>Duel and deathmatch versus bots, population-based training, 36-core server:</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel_bots --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --reward_scale=0.5 --num_workers=72 --num_envs_per_worker=32 --num_policies=8 --batch_size=2048 --benchmark=False --res_w=128 --res_h=72 --wide_aspect_ratio=False --pbt_replace_reward_gap=0.2 --pbt_replace_reward_gap_absolute=3.0 --pbt_period_env_steps=5000000 --save_milestones_sec=1800 --with_pbt=True --experiment=doom_duel_bots\npython -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_bots --train_for_seconds=3600000 --algo=APPO --use_rnn=True --gamma=0.995 --env_frameskip=2 --num_workers=80 --num_envs_per_worker=24 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --with_pbt=True --pbt_period_env_steps=5000000 --experiment=doom_deathmatch_bots\n</code></pre> <p>Duel and deathmatch self-play, PBT, 36-core server:</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_duel --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.5 --pbt_replace_reward_gap_absolute=0.35 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_duel_full\npython -m sf_examples.vizdoom.train_vizdoom --env=doom_deathmatch_full --train_for_seconds=360000 --algo=APPO --gamma=0.995 --env_frameskip=2 --use_rnn=True --num_workers=72 --num_envs_per_worker=16 --num_policies=8 --batch_size=2048 --res_w=128 --res_h=72 --wide_aspect_ratio=False --benchmark=False --pbt_replace_reward_gap=0.1 --pbt_replace_reward_gap_absolute=0.1 --pbt_period_env_steps=5000000 --with_pbt=True --pbt_start_mutation=100000000 --experiment=doom_deathmatch_full\n</code></pre> <p>Reproducing benchmarking results:</p> <p>This achieves 50K+ framerate on a 10-core machine (Intel Core i9-7900X):</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=20 --num_envs_per_worker=32 --num_policies=1 --batch_size=4096 --experiment=doom_battle_appo_fps_20_32 --res_w=128 --res_h=72 --wide_aspect_ratio=False --policy_workers_per_policy=2 --worker_num_splits=2\n</code></pre> <p>This achieves 100K+ framerate on a 36-core machine:</p> <pre><code>python -m sf_examples.vizdoom.train_vizdoom --env=doom_benchmark --algo=APPO --env_frameskip=4 --use_rnn=True --num_workers=72 --num_envs_per_worker=24 --num_policies=1 --batch_size=8192 --wide_aspect_ratio=False --experiment=doom_battle_appo_w72_v24 --policy_workers_per_policy=2\n</code></pre>"},{"location":"09-environment-integrations/vizdoom/#results","title":"Results","text":""},{"location":"09-environment-integrations/vizdoom/#reports","title":"Reports","text":"<ol> <li> <p>We reproduced the paper results in SF2 in the Battle and Battle2 and compared the results using input normalization. Input normalization has improved results in the Battle environment. This experiment with input normalization was run with <code>sf_examples.vizdoom.experiments.sf2_doom_battle_envs</code>. Note that <code>normalize_input=True</code> is set compared to the results from the paper</p> <ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/VizDoom-Battle-Environments--VmlldzoyMzcyODQx</li> </ul> </li> <li> <p>In SF2's bot environments (deathmatch_bots and duel_bots), we trained the agents against randomly generated bots as opposed to a curriculum of increasing bot difficulty. This is because the latest version of ViZDoom environment no longer provides the bots used in the curriculum, and SF2 no longer requires the curriculum to train properly. However, due to the differences in bot difficulty, the current training results are no longer comparable to the paper. An example training curve on deathmatch_bots with the same parameters as in the paper is shown below. Additionally, the report includes 8 agents trained using PBT against duel-bots with normalization and we were able to get better results than the Sample Factory paper:</p> <ul> <li>https://wandb.ai/andrewzhang505/sample_factory/reports/ViZDoom-Bots--VmlldzoyNzY2NDI1</li> </ul> </li> <li> <p>We also trained in the <code>doom_duel</code> multi-agent environment using self play. The training metrics of the experiment can be found on the Hugging Face Hub: https://huggingface.co/andrewzhang505/doom-duel-selfplay/tensorboard</p> <ul> <li>The reward scaling done by PBT can be found under <code>zz_pbt</code>. For example in this experiment, the reward scaling related to damage dealt <code>rew_DAMAGECOUNT_0</code>    increase more than 10x from 0.01 to around 0.15 at max.</li> <li>The <code>true_objective</code> reported corresponds to the fraction of matches won. In this experiment, the agents performed fairly equally as seen under <code>policy_stats/avg_true_objective</code> as agents rarely win over 60% of matches.</li> </ul> </li> </ol>"},{"location":"09-environment-integrations/vizdoom/#models","title":"Models","text":"<p>The models below are the best models from the input normalization experiment above. The evaluation metrics here are obtained by running the model 10 times.  </p> Environment HuggingFace Hub Models Evaluation Metrics Battle https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle 59.37 \u00b1 3.93 Battle2 https://huggingface.co/andrewzhang505/sample-factory-2-doom-battle2 36.40 \u00b1 4.20 Deathmatch-Bots https://huggingface.co/andrewzhang505/doom_deathmatch_bots 85.66 \u00b1 28.53 Duel-Bots https://huggingface.co/andrewzhang505/doom_duel_bots_pbt 55.39 \u00b1 17.13 Duel https://huggingface.co/andrewzhang505/doom-duel-selfplay"},{"location":"09-environment-integrations/vizdoom/#videos","title":"Videos","text":""},{"location":"09-environment-integrations/vizdoom/#doom-battle","title":"Doom Battle","text":""},{"location":"09-environment-integrations/vizdoom/#doom-battle2","title":"Doom Battle2","text":""},{"location":"09-environment-integrations/vizdoom/#doom-deathmatch-bots","title":"Doom Deathmatch Bots","text":""},{"location":"09-environment-integrations/vizdoom/#doom-duel-bots-pbt","title":"Doom Duel Bots PBT","text":""},{"location":"09-environment-integrations/vizdoom/#doom-duel-multi-agent","title":"Doom Duel Multi-Agent","text":""},{"location":"10-huggingface/huggingface/","title":"Hugging Face \ud83e\udd17 Hub","text":"<p>Sample Factory has integrations with \ud83e\udd17 Hugging Face Hub to push models with evaluation results and training metrics to the hub. </p>"},{"location":"10-huggingface/huggingface/#setting-up","title":"Setting Up","text":"<p>The Hugging Face Hub requires <code>git lfs</code> to download model files.</p> <pre><code>sudo apt install git-lfs\ngit lfs install\n</code></pre> <p>To upload files to the Hugging Face Hub, you need to sign up and log in to your Hugging Face account with:</p> <pre><code>huggingface-cli login\n</code></pre> <p>As part of the <code>huggingface-cli login</code>, you should generate a token with write access at https://huggingface.co/settings/tokens</p>"},{"location":"10-huggingface/huggingface/#downloading-models","title":"Downloading Models","text":""},{"location":"10-huggingface/huggingface/#using-the-load_from_hub-scipt","title":"Using the load_from_hub Scipt","text":"<p>To download a model from the Hugging Face Hub to use with Sample-Factory, use the <code>load_from_hub</code> script:</p> <pre><code>python -m sample_factory.huggingface.load_from_hub -r &lt;HuggingFace_repo_id&gt; -d &lt;train_dir_path&gt;\n</code></pre> <p>The command line arguments are:</p> <ul> <li> <p><code>-r</code>: The repo ID for the HF repository to download. The repo ID should be in the format <code>&lt;username&gt;/&lt;repo_name&gt;</code></p> </li> <li> <p><code>-d</code>: An optional argument to specify the directory to save the experiment to. Defaults to <code>./train_dir</code> which will save the repo to <code>./train_dir/&lt;repo_name&gt;</code></p> </li> </ul>"},{"location":"10-huggingface/huggingface/#download-model-repository-directly","title":"Download Model Repository Directly","text":"<p>Hugging Face repositories can be downloaded directly using <code>git clone</code>:</p> <pre><code>git clone &lt;URL of Hugging Face Repo&gt;\n</code></pre>"},{"location":"10-huggingface/huggingface/#using-downloaded-models-with-sample-factory","title":"Using Downloaded Models with Sample Factory","text":"<p>After downloading the model, you can run the models in the repo with the enjoy script corresponding to your environment. For example, if you are downloading a <code>mujoco-ant</code> model, it can be run with:</p> <pre><code>python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=&lt;repo_name&gt; --train_dir=./train_dir\n</code></pre> <p>Note, you may have to specify the <code>--train_dir</code> if your local train_dir has a different path than the one in the <code>config.json</code></p>"},{"location":"10-huggingface/huggingface/#uploading-models","title":"Uploading Models","text":""},{"location":"10-huggingface/huggingface/#using-enjoypy","title":"Using enjoy.py","text":"<p>You can upload your models to the Hub using your environment's <code>enjoy</code> script with the <code>--push_to_hub</code> flag. Uploading using <code>enjoy</code> can also generate evaluation metrics and a replay video.</p> <p>The evaluation metrics are generated by running your model on the specified environment for a number of episodes and reporting the mean and std reward of those runs.</p> <p>Other relevant command line arguments are:</p> <ul> <li> <p><code>--hf_repository</code>: The repository to push to. Must be of the form <code>&lt;username&gt;/&lt;repo_name&gt;</code>. The model will be saved to <code>https://huggingface.co/&lt;username&gt;/&lt;repo_name&gt;</code></p> </li> <li> <p><code>--max_num_episodes</code>: Number of episodes to evaluate on before uploading. Used to generate evaluation metrics. It is recommended to use multiple episodes to generate an accurate mean and std.</p> </li> <li> <p><code>--max_num_frames</code>: Number of frames to evaluate on before uploading. An alternative to <code>max_num_episodes</code></p> </li> <li> <p><code>--no_render</code>: A flag that disables rendering and showing the environment steps. It is recommended to set this flag to speed up the evaluation process.</p> </li> </ul> <p>You can also save a video of the model during evaluation to upload to the hub with the <code>--save_video</code> flag</p> <ul> <li> <p><code>--video_frames</code>: The number of frames to be rendered in the video. Defaults to -1 which renders an entire episode</p> </li> <li> <p><code>--video_name</code>: The name of the video to save as. If <code>None</code>, will save to <code>replay.mp4</code> in your experiment directory</p> </li> </ul> <p>Also, you can include information in the Hugging Face Hub model card for how to train and enjoy using this model. These parameters are optional:</p> <ul> <li> <p><code>--train_script</code>: The module path for training this model</p> </li> <li> <p><code>--enjoy_script</code>: The module path for enjoying this model</p> </li> </ul> <p>For example:</p> <pre><code>python -m sf_examples.mujoco.enjoy_mujoco --algo=APPO --env=mujoco_ant --experiment=&lt;repo_name&gt; --train_dir=./train_dir --max_num_episodes=10 --push_to_hub --hf_repository=&lt;username&gt;/&lt;hf_repo_name&gt; --save_video --no_render --enjoy_script=sf_examples.mujoco.enjoy_mujoco --train_script=sf_examples.mujoco.train_mujoco\n</code></pre>"},{"location":"10-huggingface/huggingface/#using-the-push_to_hub-script","title":"Using the push_to_hub Script","text":"<p>If you want to upload without generating evaluation metrics or a replay video, you can use the <code>push_to_hub</code> script:</p> <pre><code>python -m sample_factory.huggingface.push_to_hub -r &lt;hf_username&gt;/&lt;hf_repo_name&gt; -d &lt;experiment_dir_path&gt;\n</code></pre> <p>The command line arguments are:</p> <ul> <li> <p><code>-r</code>: The repo_id to save on HF Hub. This is the same as <code>hf_repository</code> in the enjoy script and must be in the form <code>&lt;hf_username&gt;/&lt;hf_repo_name&gt;</code></p> </li> <li> <p><code>-d</code>: The full path to your experiment directory to upload</p> </li> </ul> <p>The optional arguments of <code>--train_script</code> and <code>--enjoy_script</code> can also be used. See the above section for more details</p>"},{"location":"11-release-notes/release-notes/","title":"Recent releases","text":""},{"location":"11-release-notes/release-notes/#v211","title":"v2.1.1","text":"<ul> <li>Windows support in serial mode (do not require <code>faster-fifo</code> on Windows)</li> <li>Allow Torch 2 (versions 2.0.1+ seem to work fine)</li> </ul>"},{"location":"11-release-notes/release-notes/#v210","title":"v2.1.0","text":"<ul> <li>Use <code>gymnasium</code> instead of <code>gym</code> dependency. See https://gymnasium.farama.org/content/basic_usage/ New compatibility layer added to automatically convert legacy <code>gym</code> environments to <code>gymnasium</code> environments. Still, it is likely that some of the user code will need to be manually updated to use <code>gymnasium</code> instead of <code>gym</code>. Apologies for the inconvenience, but hopefully this is the last major change before Gym API finally stabilizes.</li> </ul>"},{"location":"11-release-notes/release-notes/#v203","title":"v2.0.3","text":"<ul> <li>Added cfg parameters <code>--lr_adaptive_min</code> and <code>--lr_adaptive_max</code> to control the minimum and maximum adaptive learning rate</li> <li>Added Brax environment support + custom brax renderer for enjoy scripts</li> <li>Automatically set <code>--recurrence</code> based on feed-forward vs RNN training</li> <li>Added <code>--enjoy_script</code> and <code>--train_script</code> for generating the model card when uploading to the Hugging Face Hub (thank you Andrew!)</li> <li>Fixed video name when generating Hugging Face model card</li> <li>Fixed small DMLab-related bug (thank you Lucy!)</li> </ul>"},{"location":"11-release-notes/release-notes/#v202","title":"v2.0.2","text":"<ul> <li><code>cfg.json</code> renamed to <code>config.json</code> for consistency with other HuggingFace integrations</li> <li>We can still load from legacy checkpoints (<code>cfg.json</code> will be renamed to <code>config.json</code>)</li> <li>Fixed a bug in enjoy.py with multi-agent envs</li> </ul>"},{"location":"11-release-notes/release-notes/#v201","title":"v2.0.1","text":"<ul> <li>Added MuJoCo &amp; IsaacGym examples to the PyPI package</li> <li>Added missing <code>__init__.py</code> files</li> </ul>"},{"location":"11-release-notes/release-notes/#v200","title":"v2.0.0","text":"<p>Major update, adds new functionality, changes API and configuration parameters</p> <ul> <li>Major API update, codebase rewritten from scratch for better maintainability and clarity</li> <li>Synchronous and asynchronous training modes</li> <li>Serial and parallel execution modes</li> <li>Support for vectorized and GPU-accelerated environments in batched sampling mode</li> <li>Integration with Hugging Face Hub</li> <li>New environment integrations, CI, and 40+ documentation pages</li> </ul> <p>See v1 to v2 transition guide for details.</p>"},{"location":"11-release-notes/release-notes/#v11214","title":"v1.121.4","text":"<ul> <li>Support Weights and Biases (see section \"WandB support\")</li> <li>More configurable population-based training:  can set from command line whether or not to mutate gamma, plus the perturbation magnitude for all float hyperparams can also be set from command line: <pre><code>--pbt_optimize_gamma: Whether to optimize gamma, discount factor, or not (experimental) (default: False)\n--pbt_perturb_min: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.05)\n--pbt_perturb_max: When PBT mutates a float hyperparam, it samples the change magnitude randomly from the uniform distribution [pbt_perturb_min, pbt_perturb_max] (default: 1.5)\n</code></pre></li> </ul>"},{"location":"11-release-notes/release-notes/#v11213","title":"v1.121.3","text":"<ul> <li>Fixed a small bug related to population-based training (a reward shaping dictionary was assumed to be a flat dict, while it could be a nested dict in some envs)</li> </ul>"},{"location":"11-release-notes/release-notes/#v11212","title":"v1.121.2","text":"<ul> <li>Fixed a bug that prevented Vizdoom *.cfg and *.wad files from being copied to site-packages during installation from PyPI</li> <li>Added example on how to use custom Vizdoom envs without modifying the source code (<code>sample_factory_examples/train_custom_vizdoom_env.py</code>)</li> </ul>"},{"location":"11-release-notes/release-notes/#v11210","title":"v1.121.0","text":"<ul> <li> <p>Added fixed KL divergence penalty as in https://arxiv.org/pdf/1707.06347.pdf  Its usage is highly encouraged in environments with continuous action spaces (i.e. set --kl_loss_coeff=1.0). Otherwise numerical instabilities can occur in certain environments, especially when the policy lag is high</p> </li> <li> <p>More summaries related to the new loss</p> </li> </ul>"},{"location":"11-release-notes/release-notes/#v11202","title":"v1.120.2","text":"<ul> <li>More improvements and fixes in runner interface, including support for NGC cluster</li> </ul>"},{"location":"11-release-notes/release-notes/#v11201","title":"v1.120.1","text":"<ul> <li>Runner interface improvements for Slurm</li> </ul>"},{"location":"11-release-notes/release-notes/#v11200","title":"v1.120.0","text":"<ul> <li>Support inactive agents. To deactivate an agent for a portion of the episode the environment should return <code>info={'is_active': False}</code> for the inactive agent. Useful for environments such as hide-n-seek.</li> <li>Improved memory consumption and performance with better shared memory management.</li> <li>Experiment logs are now saved into the experiment folder as <code>sf_log.txt</code></li> <li>DMLab-related bug fixes (courtesy of @donghoonlee04 and @sungwoong. Thank you!)</li> </ul>"},{"location":"12-community/citation/","title":"Citation","text":"<p>If you use this repository in your work or otherwise wish to cite it, please make reference to our ICML2020 paper.</p> <pre><code>@inproceedings{petrenko2020sf,\n  author    = {Aleksei Petrenko and\n               Zhehui Huang and\n               Tushar Kumar and\n               Gaurav S. Sukhatme and\n               Vladlen Koltun},\n  title     = {Sample Factory: Egocentric 3D Control from Pixels at 100000 {FPS}\n               with Asynchronous Reinforcement Learning},\n  booktitle = {Proceedings of the 37th International Conference on Machine Learning,\n               {ICML} 2020, 13-18 July 2020, Virtual Event},\n  series    = {Proceedings of Machine Learning Research},\n  volume    = {119},\n  pages     = {7652--7662},\n  publisher = {{PMLR}},\n  year      = {2020},\n  url       = {http://proceedings.mlr.press/v119/petrenko20a.html},\n  biburl    = {https://dblp.org/rec/conf/icml/PetrenkoHKSK20.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> <p>For questions, issues, inquiries please join Discord. Github issues and pull requests are welcome.</p>"},{"location":"12-community/contribution/","title":"Contribute to SF","text":""},{"location":"12-community/contribution/#how-to-contribute-to-sample-factory","title":"How to contribute to Sample Factory?","text":"<p>Sample Factory is an open source project, so all contributions and suggestions are welcome.</p> <p>You can contribute in many different ways: giving ideas, answering questions, reporting bugs, proposing enhancements,  improving the documentation, fixing bugs.</p> <p>Huge thanks in advance to every contributor!</p>"},{"location":"12-community/contribution/#how-to-work-on-an-open-issue","title":"How to work on an open Issue?","text":"<p>You have the list of open Issues at: https://github.com/alex-petrenko/sample-factory/issues</p> <p>Some of them may have the label <code>help wanted</code>: that means that any contributor is welcomed!</p> <p>If you would like to work on any of the open Issues:</p> <ol> <li> <p>Make sure it is not already assigned to someone else. You have the assignee (if any) on the top of the right column of the Issue page.</p> </li> <li> <p>You can self-assign it by commenting on the Issue page with one of the keywords: <code>#take</code> or <code>#self-assign</code>.</p> </li> <li> <p>Work on your self-assigned issue and eventually create a Pull Request.</p> </li> </ol>"},{"location":"12-community/contribution/#how-to-create-a-pull-request","title":"How to create a Pull Request?","text":"<ol> <li> <p>Fork the repository by clicking on the 'Fork' button on the repository's page. This creates a copy of the code under your GitHub user account.</p> </li> <li> <p>Clone your fork to your local disk, and add the base repository as a remote:</p> <pre><code>git clone git@github.com:&lt;your Github handle&gt;/sample-factory.git\ncd sample-factory\ngit remote add upstream https://github.com/alex-petrenko/sample-factory.git\n</code></pre> </li> <li> <p>Create a new branch to hold your development changes:</p> <pre><code>git checkout -b a-descriptive-name-for-my-changes\n</code></pre> <p>do not work on the <code>main</code> branch.</p> </li> <li> <p>Set up a development environment by running the following command in a virtual (or conda) environment:</p> <pre><code>pip install -e .[dev]\n</code></pre> </li> </ol> <p>(If sample-factory was already installed in the virtual environment, remove    it with <code>pip uninstall sample-factory</code> before reinstalling it in editable    mode with the <code>-e</code> flag.)</p> <ol> <li> <p>Develop the features on your branch.</p> </li> <li> <p>Format your code. Run black and isort so that your newly added files look nice with the following command:</p> <pre><code>make format\n</code></pre> <p>If you want to enable auto format checking before every commit, you can run the following command:  <pre><code>pre-commit install\n</code></pre></p> </li> </ol> <p>(Optional) to update pre-commit hooks, run the following command:     <pre><code>pre-commit autoupdate\n</code></pre></p> <ol> <li> <p>Run unittests with the following command:     <pre><code>make test\n</code></pre></p> </li> <li> <p>Once you're happy with your files, add your changes and make a commit to record your changes locally:</p> <pre><code>git add sample-factory/&lt;your_dataset_name&gt;\ngit commit\n</code></pre> <p>It is a good idea to sync your copy of the code with the original repository regularly. This way you can quickly account for changes:</p> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> </li> </ol> <p>Push the changes to your account using:</p> <pre><code>git push -u origin a-descriptive-name-for-my-changes\n</code></pre> <ol> <li>Once you are satisfied, go the webpage of your fork on GitHub. Click on \"Pull request\" to send your to the project maintainers for review.</li> </ol> <p>See also additional notes on how to contribute to the documentation website.</p>"},{"location":"12-community/doc-contribution/","title":"Doc Contribution","text":"<ul> <li> <p>Clone the repo. You should be in the root folder containing \u2018docs\u2019, \u2018mkdocs.yml\u2019 config file, and \u2018docs.yml\u2019 github actions file.</p> </li> <li> <p>Install dev dependencies (includes <code>mkdocs</code> deps):</p> </li> </ul> <pre><code>pip install -e .[dev]\n</code></pre> <ul> <li>Serve the website locally</li> </ul> <pre><code>make docs-serve\n</code></pre> <p>you should see the website on your localhost port now.</p> <ul> <li>Modify or create markdown files</li> <li>modify / create your markdown files in \u2018docs\u2019 folder.</li> <li>add your markdown path in the \u2018nav\u2019 section of \u2018mkdocs.yml\u2019.</li> </ul> <p>Example folder-yml correspondence:</p> <p> </p> <ul> <li>Commit and push your changes to remote repo. Github actions will automatically push your changes to your github pages website.</li> </ul>"}]}